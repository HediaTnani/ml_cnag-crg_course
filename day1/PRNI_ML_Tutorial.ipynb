{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jkmejVSm0qtD"
   },
   "source": [
    "# PRNI Summer School 2020 - Tutorial for Machine Learning\n",
    "\n",
    "**Date**: Sep-14th-2020\n",
    "\n",
    "\n",
    "**Live tutorial time**: 1:00pm - 2:30pm (Central European Summer Time, UTC +2) \n",
    "\n",
    "**Live tutorial link**: Please refer to your slack tutorial group\n",
    "\n",
    "**Wrap-up for tutorial**: A wrap-up lecture will be held by the end of the tutorial at 2:30pm.\n",
    "\n",
    "\n",
    "\n",
    "**Lecturer**: Prof. Dr. Sebastian Tschiatschek \n",
    "\n",
    "**Tutor**: Jiachen Xu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jvjfRHwGF0bT"
   },
   "source": [
    "# General Instructions\n",
    "\n",
    "1. Please try to solve the taks without refering to the pseudocode and solutions.\n",
    "2. The pseudocode is provided in **Double click to show hint code hint code**. Please note that the provided pseudocode is not necessarily to be the unique solution.\n",
    "3. The solution is provided in **Double click to show solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rDJIly0yz8PR"
   },
   "source": [
    "# Basics of sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6dKbss7cz8PS"
   },
   "source": [
    "The goal of this task is to get used to the basics of scikit-learn and train and compare a few different classifiers/regressors and tune their hyper-parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SIiOKLOIz8PT"
   },
   "source": [
    "## Load the UCI breast cancer dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "09l-vJu7z8PT"
   },
   "source": [
    "Use the scikit-learn function `sklearn.datasets.load_breast_cancer()` to load the breast cancer dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nilearn in c:\\users\\tahnee\\anaconda3\\lib\\site-packages (0.6.2)\n",
      "Requirement already satisfied: numpy>=1.11 in c:\\users\\tahnee\\anaconda3\\lib\\site-packages (from nilearn) (1.16.4)\n",
      "Requirement already satisfied: sklearn in c:\\users\\tahnee\\anaconda3\\lib\\site-packages (from nilearn) (0.0)\n",
      "Requirement already satisfied: scipy>=0.19 in c:\\users\\tahnee\\anaconda3\\lib\\site-packages (from nilearn) (1.2.1)\n",
      "Requirement already satisfied: scikit-learn>=0.19 in c:\\users\\tahnee\\anaconda3\\lib\\site-packages (from nilearn) (0.21.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\tahnee\\anaconda3\\lib\\site-packages (from nilearn) (0.13.2)\n",
      "Requirement already satisfied: nibabel>=2.0.2 in c:\\users\\tahnee\\anaconda3\\lib\\site-packages (from nilearn) (3.1.1)\n",
      "Requirement already satisfied: packaging>=14.3 in c:\\users\\tahnee\\anaconda3\\lib\\site-packages (from nibabel>=2.0.2->nilearn) (19.0)\n",
      "Requirement already satisfied: six in c:\\users\\tahnee\\anaconda3\\lib\\site-packages (from packaging>=14.3->nibabel>=2.0.2->nilearn) (1.12.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\tahnee\\anaconda3\\lib\\site-packages (from packaging>=14.3->nibabel>=2.0.2->nilearn) (2.4.0)\n",
      "Requirement already satisfied: brainspace in c:\\users\\tahnee\\anaconda3\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in c:\\users\\tahnee\\anaconda3\\lib\\site-packages (from brainspace) (1.2.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\tahnee\\anaconda3\\lib\\site-packages (from brainspace) (6.1.0)\n",
      "Requirement already satisfied: matplotlib>=2.0.0 in c:\\users\\tahnee\\anaconda3\\lib\\site-packages (from brainspace) (3.1.0)\n",
      "Requirement already satisfied: vtk>=8.1.0 in c:\\users\\tahnee\\anaconda3\\lib\\site-packages (from brainspace) (9.0.1)\n",
      "Requirement already satisfied: nibabel in c:\\users\\tahnee\\anaconda3\\lib\\site-packages (from brainspace) (3.1.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\tahnee\\anaconda3\\lib\\site-packages (from brainspace) (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.11.0 in c:\\users\\tahnee\\anaconda3\\lib\\site-packages (from brainspace) (1.16.4)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\tahnee\\anaconda3\\lib\\site-packages (from brainspace) (0.21.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\tahnee\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->brainspace) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\tahnee\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->brainspace) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\tahnee\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->brainspace) (2.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\tahnee\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->brainspace) (2.8.0)\n",
      "Requirement already satisfied: packaging>=14.3 in c:\\users\\tahnee\\anaconda3\\lib\\site-packages (from nibabel->brainspace) (19.0)\n",
      "Requirement already satisfied: pytz>=2011k in c:\\users\\tahnee\\anaconda3\\lib\\site-packages (from pandas->brainspace) (2019.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\tahnee\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.0->brainspace) (0.13.2)\n",
      "Requirement already satisfied: six in c:\\users\\tahnee\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib>=2.0.0->brainspace) (1.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tahnee\\anaconda3\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib>=2.0.0->brainspace) (41.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nilearn\n",
    "!pip install brainspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "hc7xIGZF2G_T"
   },
   "outputs": [],
   "source": [
    "#@title Double click to show hint code\n",
    "\n",
    "import sklearn.datasets\n",
    "data = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "RQ52XyZPz8PU"
   },
   "outputs": [],
   "source": [
    "#@title Double click to show solution \n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "data = sklearn.datasets.load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VUaZbDtrz8PX"
   },
   "source": [
    "## Read the description of the dataset and the features used for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v7ZyR1YRz8PY"
   },
   "source": [
    "Study the data structure returned by `sklearn.datasets.load_breast_cancer()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WzMUqeuD3uaj"
   },
   "source": [
    "**Hint**: The information is encapsulated in the attribute `DESCR` and `feature_names`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "AIpQuQuTz8PY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _breast_cancer_dataset:\n",
      "\n",
      "Breast cancer wisconsin (diagnostic) dataset\n",
      "--------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 569\n",
      "\n",
      "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
      "\n",
      "    :Attribute Information:\n",
      "        - radius (mean of distances from center to points on the perimeter)\n",
      "        - texture (standard deviation of gray-scale values)\n",
      "        - perimeter\n",
      "        - area\n",
      "        - smoothness (local variation in radius lengths)\n",
      "        - compactness (perimeter^2 / area - 1.0)\n",
      "        - concavity (severity of concave portions of the contour)\n",
      "        - concave points (number of concave portions of the contour)\n",
      "        - symmetry \n",
      "        - fractal dimension (\"coastline approximation\" - 1)\n",
      "\n",
      "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
      "        largest values) of these features were computed for each image,\n",
      "        resulting in 30 features.  For instance, field 3 is Mean Radius, field\n",
      "        13 is Radius SE, field 23 is Worst Radius.\n",
      "\n",
      "        - class:\n",
      "                - WDBC-Malignant\n",
      "                - WDBC-Benign\n",
      "\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ===================================== ====== ======\n",
      "                                           Min    Max\n",
      "    ===================================== ====== ======\n",
      "    radius (mean):                        6.981  28.11\n",
      "    texture (mean):                       9.71   39.28\n",
      "    perimeter (mean):                     43.79  188.5\n",
      "    area (mean):                          143.5  2501.0\n",
      "    smoothness (mean):                    0.053  0.163\n",
      "    compactness (mean):                   0.019  0.345\n",
      "    concavity (mean):                     0.0    0.427\n",
      "    concave points (mean):                0.0    0.201\n",
      "    symmetry (mean):                      0.106  0.304\n",
      "    fractal dimension (mean):             0.05   0.097\n",
      "    radius (standard error):              0.112  2.873\n",
      "    texture (standard error):             0.36   4.885\n",
      "    perimeter (standard error):           0.757  21.98\n",
      "    area (standard error):                6.802  542.2\n",
      "    smoothness (standard error):          0.002  0.031\n",
      "    compactness (standard error):         0.002  0.135\n",
      "    concavity (standard error):           0.0    0.396\n",
      "    concave points (standard error):      0.0    0.053\n",
      "    symmetry (standard error):            0.008  0.079\n",
      "    fractal dimension (standard error):   0.001  0.03\n",
      "    radius (worst):                       7.93   36.04\n",
      "    texture (worst):                      12.02  49.54\n",
      "    perimeter (worst):                    50.41  251.2\n",
      "    area (worst):                         185.2  4254.0\n",
      "    smoothness (worst):                   0.071  0.223\n",
      "    compactness (worst):                  0.027  1.058\n",
      "    concavity (worst):                    0.0    1.252\n",
      "    concave points (worst):               0.0    0.291\n",
      "    symmetry (worst):                     0.156  0.664\n",
      "    fractal dimension (worst):            0.055  0.208\n",
      "    ===================================== ====== ======\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
      "\n",
      "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
      "\n",
      "    :Donor: Nick Street\n",
      "\n",
      "    :Date: November, 1995\n",
      "\n",
      "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
      "https://goo.gl/U2Uwz2\n",
      "\n",
      "Features are computed from a digitized image of a fine needle\n",
      "aspirate (FNA) of a breast mass.  They describe\n",
      "characteristics of the cell nuclei present in the image.\n",
      "\n",
      "Separating plane described above was obtained using\n",
      "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
      "Construction Via Linear Programming.\" Proceedings of the 4th\n",
      "Midwest Artificial Intelligence and Cognitive Science Society,\n",
      "pp. 97-101, 1992], a classification method which uses linear\n",
      "programming to construct a decision tree.  Relevant features\n",
      "were selected using an exhaustive search in the space of 1-4\n",
      "features and 1-3 separating planes.\n",
      "\n",
      "The actual linear program used to obtain the separating plane\n",
      "in the 3-dimensional space is that described in:\n",
      "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
      "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
      "Optimization Methods and Software 1, 1992, 23-34].\n",
      "\n",
      "This database is also available through the UW CS ftp server:\n",
      "\n",
      "ftp ftp.cs.wisc.edu\n",
      "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
      "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
      "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
      "     San Jose, CA, 1993.\n",
      "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
      "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
      "     July-August 1995.\n",
      "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
      "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
      "     163-171.\n",
      "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n"
     ]
    }
   ],
   "source": [
    "#@title Double click to show solution \n",
    "print(data.DESCR)\n",
    "print(data.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['malignant', 'benign']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.target[[10,50,85]]\n",
    "list(data.target_names)\n",
    "\n",
    "#class of malignant and benign given "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RDkWZF0pz8Pb"
   },
   "source": [
    "## Identify the number of samples and features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qz6Q_evh513W"
   },
   "source": [
    "**Hint:** The data matrix has two dimensions. Please refer to the description to figure out which dimension is the samples and which represents the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "qppGTTRp4gab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 569\n",
      "Number of features: 30\n"
     ]
    }
   ],
   "source": [
    "#@title Double click to show hint code\n",
    "\n",
    "n_samples = 569\n",
    "n_features = 30\n",
    "print(\"Number of samples: %d\" % n_samples)\n",
    "print(\"Number of features: %d\" % n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "G8onIuLkz8Pc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 569\n",
      "Number of features: 30\n"
     ]
    }
   ],
   "source": [
    "#@title Double click to show solution\n",
    "\n",
    "n_samples = data.data.shape[0]#columns \n",
    "n_features = data.data.shape[1] #rows \n",
    "print(\"Number of samples: %d\" % n_samples)\n",
    "print(\"Number of features: %d\" % n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "breast cancer has a data attribute - first dimension number of samples and second dimension number of features (30 numeric attributes)\n",
    "\n",
    "How many are from one calls and how many from the other - predict the more prominent class. Compute fraction of samples from class 1. Target (1 for 1 class and 0 for the other one) - total number of targets \n",
    "best prediction: maximizes probabilty of predicted class \n",
    "class 1:62%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aveb531Sz8Pf"
   },
   "source": [
    "## Determine the best constant prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9LGZT2xfz8Pf"
   },
   "source": [
    "The breast dataset describes a two class classification problem. What is the best constant prediction for this dataset in terms of the 0/1-loss?\n",
    "\n",
    "**Hint:** Compute the probablity of each class first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "jRbIEcAW43SM"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "%d format: a number is required, not ellipsis",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-dbe50caf36cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprob_class_0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mbest_constant_prediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m...\u001b[0m \u001b[1;31m# func(prob_class_1, prob_class_2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Best prediction: Class %d\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbest_constant_prediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: %d format: a number is required, not ellipsis"
     ]
    }
   ],
   "source": [
    "#@title Double click to show hint code\n",
    "\n",
    "prob_class_1 = ...\n",
    "prob_class_0 = ...\n",
    "best_constant_prediction = ... # func(prob_class_1, prob_class_2)\n",
    "print(\"Best prediction: Class %d\" % best_constant_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "9feKOMfrz8Pg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best prediction: Class 1\n"
     ]
    }
   ],
   "source": [
    "#@title Double click to show solution\n",
    "\n",
    "import numpy as np\n",
    "prob_class_1 = np.sum(data.target) / len(data.target)#calculate percentage \n",
    "prob_class_0 = 1 - prob_class_1 #calculate percentage \n",
    "best_constant_prediction = np.argmax([prob_class_0, prob_class_1]) #gives the the largest argument (only gives the index, so in this case 1 (starts with 0))\n",
    "print(\"Best prediction: Class %d\" % best_constant_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training and testing data sets \n",
    "model_selection sub package \n",
    "function: train_test_split \n",
    "\n",
    "print('train: %d' % len(y_train))\n",
    "how many samples were used for training \n",
    "for all see below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bpc8snkFz8Pi"
   },
   "source": [
    "## Creating a training and a test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t8lGGX1Xz8Pj"
   },
   "source": [
    "Use the scikit-learn function `sklearn.model_selection.train_test_split` to create a training and test dataset. Use 70% of the data for the training data and the remaining 30% of the data for the test set.\n",
    "\n",
    "**Hint:** Read the function doc [here.](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection as sk\n",
    "X_train = sk.train_test_spl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "USM6cJ5A81QT"
   },
   "outputs": [],
   "source": [
    "#@title Double click to show hint code\n",
    "\n",
    "X_train, X_test, y_train, y_test = ... # func(data, label, train=70%, test=30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "Eirkl73cz8Pj"
   },
   "outputs": [],
   "source": [
    "#@title Double click to show solution\n",
    "\n",
    "import sklearn.model_selection\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(data.data, data.target, test_size=0.3, train_size=None)\n",
    "#need to put them in correct order, in the order that the output is (given in documentation)\n",
    "# data.data just sepcifies the data (the x)\n",
    "# data.target specifies the Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OVAVtyOGz8Pm"
   },
   "source": [
    "## Investigate the feature statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xS3RT3Bcz8Pn"
   },
   "source": [
    "Compute the mean, standard deviation and range of the individual features of the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BAKm-KTk-r8M"
   },
   "source": [
    "**Hint**: \n",
    "1.   Use package `numpy` to compute these statistics. To find the appropriate functions, read the related doc [here.](https://numpy.org/doc/stable/reference/routines.statistics.html)\n",
    "2.   The range of feature indicates the minimum and maximum value of this feature.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.635678391959799\n",
      "0.48123941438249124\n",
      "0.635678391959799 0.48123941438249124 0 1\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "mean = numpy.mean(y_train)\n",
    "std = numpy.std(y_train)\n",
    "print(mean)\n",
    "print(std)\n",
    "mmin = numpy.nanmin(y_train)\n",
    "mmax = numpy.nanmax(y_train)\n",
    "print (mean, std, mmin, mmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "_tEDBl20_EMW"
   },
   "outputs": [],
   "source": [
    "#@title Double click to show hint code\n",
    "\n",
    "# for each_feature in feature_list:\n",
    "    mean = ... # func(each_feature)\n",
    "    std = ... # func(each_feature)\n",
    "    mmin = ... # func(each_feature)\n",
    "    mmax = ... # func(each_feature)\n",
    "    print(\"Feature %d: mean=%f, std=%f, min=%f, max=%f\" % (each_feature, mean, std, mmin, mmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "2tHemGVxz8Pn",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 0: mean=14.130583, std=3.513679, min=6.981000, max=28.110000\n",
      "Feature 1: mean=19.044070, std=4.133749, min=9.710000, max=33.810000\n",
      "Feature 2: mean=91.966382, std=24.161267, min=43.790000, max=188.500000\n",
      "Feature 3: mean=655.079648, std=347.922882, min=143.500000, max=2499.000000\n",
      "Feature 4: mean=0.096561, std=0.013986, min=0.062510, max=0.163400\n",
      "Feature 5: mean=0.103228, std=0.049860, min=0.019380, max=0.311400\n",
      "Feature 6: mean=0.087724, std=0.077369, min=0.000000, max=0.426800\n",
      "Feature 7: mean=0.048883, std=0.038388, min=0.000000, max=0.201200\n",
      "Feature 8: mean=0.181341, std=0.026911, min=0.106000, max=0.304000\n",
      "Feature 9: mean=0.062772, std=0.006737, min=0.050250, max=0.095020\n",
      "Feature 10: mean=0.406670, std=0.278126, min=0.115300, max=2.873000\n",
      "Feature 11: mean=1.209885, std=0.570172, min=0.362100, max=4.885000\n",
      "Feature 12: mean=2.882638, std=2.040939, min=0.757000, max=21.980000\n",
      "Feature 13: mean=40.518008, std=43.879257, min=6.802000, max=525.600000\n",
      "Feature 14: mean=0.006973, std=0.002840, min=0.001713, max=0.023330\n",
      "Feature 15: mean=0.025105, std=0.017630, min=0.002252, max=0.135400\n",
      "Feature 16: mean=0.031457, std=0.029087, min=0.000000, max=0.396000\n",
      "Feature 17: mean=0.011784, std=0.006156, min=0.000000, max=0.052790\n",
      "Feature 18: mean=0.020557, std=0.007739, min=0.007882, max=0.056280\n",
      "Feature 19: mean=0.003768, std=0.002640, min=0.000968, max=0.029840\n",
      "Feature 20: mean=16.240578, std=4.786118, min=7.930000, max=33.130000\n",
      "Feature 21: mean=25.311482, std=5.914026, min=12.020000, max=49.540000\n",
      "Feature 22: mean=107.149095, std=33.330390, min=50.410000, max=229.300000\n",
      "Feature 23: mean=877.751508, std=556.380626, min=185.200000, max=3432.000000\n",
      "Feature 24: mean=0.131994, std=0.022347, min=0.071170, max=0.190900\n",
      "Feature 25: mean=0.247981, std=0.142866, min=0.027290, max=0.932700\n",
      "Feature 26: mean=0.266824, std=0.194361, min=0.000000, max=0.960800\n",
      "Feature 27: mean=0.114019, std=0.065075, min=0.000000, max=0.291000\n",
      "Feature 28: mean=0.289507, std=0.059988, min=0.156500, max=0.577400\n",
      "Feature 29: mean=0.083384, std=0.016092, min=0.055040, max=0.144600\n"
     ]
    }
   ],
   "source": [
    "#@title Double click to show solution\n",
    "\n",
    "for f in range(n_features):\n",
    "    mean = np.mean(X_train[:,f]) # first row then column \n",
    "    std = np.std(X_train[:,f])\n",
    "    mmin = np.min(X_train[:,f])\n",
    "    mmax = np.max(X_train[:,f])\n",
    "    print(\"Feature %d: mean=%f, std=%f, min=%f, max=%f\" % (f, mean, std, mmin, mmax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not yet standardized data, would have an effect if you train certain models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "od309RwAz8Pq"
   },
   "source": [
    "## Train a logistic regression classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rBzqhayWz8Pq"
   },
   "source": [
    "Train a logistic regression classifier available through `sklearn.linear_model.LogisticRegression` on the training data. Compute the accuracy on the training and test set.\n",
    "\n",
    "**Hint**: \n",
    "1.   To better understand how to use `sklearn` package to train a classifier and make predictions, please refer to [here.](https://scikit-learn.org/stable/tutorial/basic/tutorial.html#learning-and-predicting) \n",
    "2.   To score the predictions, i.e., the predicted labels, there exist numerious metrics. In this tutorial, we only use `accuracy` as our metric. Please read the related doc [here](https://scikit-learn.org/stable/modules/model_evaluation.html) to find the appropriate scoring function. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model \n",
    "import sklearn.metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "1SUA8To3Bl_0"
   },
   "outputs": [],
   "source": [
    "#@title Double click to show hint code\n",
    "\n",
    "clf = ... # Instantiate a Logistic Regression classifier with solver as'liblinear'\n",
    "... # Train the LR classifier using the training dataset\n",
    "y_predict = ... # Make predictions for the training & testing dataset\n",
    "acc = ... # Use propoer scoring function to compute the accuracy of your predictions\n",
    "print(\"Accuracy on test data: %.2f %%\" % acc_train)\n",
    "print(\"Accuracy on test data: %.2f %%\" % acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "3SKko2bGz8Pq"
   },
   "outputs": [],
   "source": [
    "#@title Double click to show solution\n",
    "\n",
    "import sklearn.linear_model\n",
    "import sklearn.metrics\n",
    "clf = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=1000)#sover tells you how to solve optimization background , and how many integeration the solver can run \n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_train = clf.predict(X_train)#make predictions \n",
    "acc_train = sklearn.metrics.accuracy_score(y_train, y_pred_train)#from metrics subpackage to compute 01 accuracy to see how often predicted values are the same\n",
    "print(\"Accuracy on training data: %.2f %%\" % acc_train)\n",
    "y_pred_test = clf.predict(X_test)#same computed for the test data \n",
    "acc_test = sklearn.metrics.accuracy_score(y_test, y_pred_test)\n",
    "print(\"Accuracy on test data: %.2f %%\" % (100*acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JvoanjBGz8Pt"
   },
   "source": [
    "## Generalization of the learned classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DETWag9Mz8Pt"
   },
   "source": [
    "Train and evaluate a sequence of classifiers trained on increasing amounts of training data. To do this, \n",
    "\n",
    "\n",
    "1. Use increasing fractions of the training data (10%, 20%, ...) for learning a logistic regression classifier\n",
    "2. Evaluate it on the **fixed test data** you have created\n",
    "3. Average your results over **multiple random selections** of the training data for each fraction (average at least 10 times)\n",
    "4. Plot the results (*x-axis*: fraction of training data; *y-axis*: accuracy on training data (the fraction the model was trained on) / accuracy on test data).\n",
    "5. What do you conclude from the plot?\n",
    "\n",
    "\n",
    "**Hint**:\n",
    "2. To randomize the data or generate random indices, you can use `numpy` package which can be refered from [here.](https://docs.scipy.org/doc/numpy-1.14.0/reference/routines.random.html)\n",
    "1. To plot the figure, please use `from matplotlib import pyplot as plt` & `plt.plot()`. For more detailed usage, please refer to [here.](https://matplotlib.org/3.3.1/api/_as_gen/matplotlib.pyplot.plot.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "I9wTn6x3HWEF"
   },
   "outputs": [],
   "source": [
    "#@title Double click to show hint code\n",
    "\n",
    "\n",
    "# Step 1: Define the training fraction list, e.g., [10%, 20%, ..., 100%]\n",
    "# Step 2: Choose one fraction number from the above list.\n",
    "# Step 3: Select the training data randomly based on given fraction.\n",
    "# Step 4: Train the classifier based on randomly selected training data\n",
    "#         and evaluate the accuracy based on selected training data \n",
    "#         & fixed testing data\n",
    "# Step 5: Repeat step 3&4 for at least 10 times to get the lists of accuracies \n",
    "#         and average them to get the mean accuracy under given fraction\n",
    "# Step 6: Return to step 2 to change another fraction number\n",
    "#         until enumerating all elements in the fraction list.\n",
    "# Step 7: Plot two curves: the one is the accuracies for the training dataset,\n",
    "#         and the other one is for the fixed testing dataset.\n",
    "#         Also properly denote the two curves by using \n",
    "#         plt.plot(..., label='train'/'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "vu8uvWC7z8Pu"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-6c2d28a28a13>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mperm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mperm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfraction\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mn_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0my_pred_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0macc_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'clf' is not defined"
     ]
    }
   ],
   "source": [
    "#@title Double click to show solution\n",
    "\n",
    "n_average = 10\n",
    "train_fraction = np.linspace(0.1, 1.0, 10)\n",
    "perf_train = np.zeros(len(train_fraction))\n",
    "perf_test = np.zeros(len(train_fraction))\n",
    "n_train = X_train.shape[0]\n",
    "for idx, fraction in enumerate(train_fraction):\n",
    "    for i in range(n_average):\n",
    "        perm = np.random.permutation(n_train)\n",
    "        indices = perm[:int(fraction * n_train)]\n",
    "        clf.fit(X_train[indices], y_train[indices])\n",
    "        y_pred_train = clf.predict(X_train[indices])\n",
    "        acc_train = sklearn.metrics.accuracy_score(y_train[indices], y_pred_train)\n",
    "        y_pred_test = clf.predict(X_test)\n",
    "        acc_test = sklearn.metrics.accuracy_score(y_test, y_pred_test)\n",
    "        perf_train[idx] += acc_train\n",
    "        perf_test[idx] += acc_test\n",
    "perf_train /= n_average\n",
    "perf_test /= n_average\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.plot(train_fraction, perf_train, label=\"train\")\n",
    "plt.plot(train_fraction, perf_test, label=\"test\")\n",
    "plt.xlabel(\"Fraction of training data\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zMA_QrIZz8Px"
   },
   "source": [
    "## Advanced: Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FyWMb9j4z8Px"
   },
   "source": [
    "The breast cancer dataset consists of 30 features. We want to select a subset of the features which still yields good classification accuracy to better understand the importance of the features. We aim to to do so using **greedy forward feature selection**, i.e., we train classifiers on increasing subsets of the features, and starting from a set consisting of a single feature, add the features which increase the classification performance most iteratively. Output an order list of features (ordered by when they were added). You should be able to answer the following questions:\n",
    "\n",
    "1. What are the 5 most important features?\n",
    "2. What is the training and test accuracy of the model using the first best features, the two best features, ...?\n",
    "3. How many features are required to achieve maximum test accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "fSMsF0XnQtLD"
   },
   "outputs": [],
   "source": [
    "#@title Double click to show hint code\n",
    "\n",
    "# Step 1: Start with ONE feature to train & evaluate the classifier\n",
    "#         to get the list of accuracies with the shape of number of features\n",
    "#         until enumerating all features.\n",
    "# Step 2: Find the feature associated with best accuracy and \n",
    "#         save its index to a 'best_feature_list'\n",
    "# Step 3: Train a new classfier using the training dataset and \n",
    "#         features saved in the 'best_feature_list'\n",
    "# Step 4: Evaluate the trained classfier from step 3 with using \n",
    "#         test dataset and features saved in the 'best_feature_list' \n",
    "# Step 5: Save the best accuracy from step 2 (or 8) and accuracy\n",
    "#         from step 4 into seperate lists of accuracy,\n",
    "#         i.e., acc_train, acc_test.\n",
    "# Step 6: Use all features saved in 'best_feature_list' and \n",
    "#         append this feature matrix with ONE additional \n",
    "#         feature which does not exist in the 'best_feature_list'.\n",
    "# Step 7: Use training dataset and appended feature matrix from \n",
    "#         Step 6 to train & evaluate a new classifier.\n",
    "# Step 8: Find the new appended features associated with best \n",
    "#         accuracy computed from Step 7 and append its index \n",
    "#         to the 'best_feature_list'\n",
    "# Step 9: Repeat Step 3-8 until the length of 'best_feature_list'\n",
    "#         is equal to the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "h1ZeX6TNz8Py"
   },
   "outputs": [],
   "source": [
    "#@title Double click to show solution\n",
    "\n",
    "features = []\n",
    "performances = []\n",
    "performances_test = []\n",
    "for it in range(n_features):\n",
    "    candidates = [i for i in range(n_features) if i not in features]\n",
    "    # evaluate candidates on top of already selected features\n",
    "    performance = np.zeros(len(candidates))\n",
    "    for idx, candidate in enumerate(candidates):\n",
    "        selected = list(features)\n",
    "        selected.append(candidate)\n",
    "        X = X_train[:,selected]\n",
    "        clf.fit(X, y_train)\n",
    "        y_pred_train = clf.predict(X)\n",
    "        acc_train = sklearn.metrics.accuracy_score(y_train, y_pred_train)\n",
    "        performance[idx] = acc_train\n",
    "    \n",
    "    # select best feature to add\n",
    "    best_idx = np.argmax(performance)\n",
    "    performances.append(np.max(performance))\n",
    "    features.append(candidates[best_idx])\n",
    "    \n",
    "    # test performance\n",
    "    X = X_train[:,features]\n",
    "    clf.fit(X, y_train)\n",
    "    X = X_test[:,features]\n",
    "    y_pred_test = clf.predict(X)\n",
    "    acc_test = sklearn.metrics.accuracy_score(y_test, y_pred_test)\n",
    "    performances_test.append(acc_test)\n",
    "\n",
    "# print list of selected features\n",
    "for fidx in features:\n",
    "    print(\"Feature %02d: %s => Acc train=%.2f / Acc test=%.2f\" % (fidx, data.feature_names[fidx], performances[fidx], performances_test[fidx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "select small expressive set of features \n",
    "ask domain expert whether these features make sense \n",
    "with more features you get more noise \n",
    "and the easier it become to fit data \n",
    "loop for all feature picked and select somehow which feature improves most \n",
    "- get list from most to least important feature - one already gives you quite good performance - in this case even drops if you select more features \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9foMti6Cz8P0"
   },
   "source": [
    "## Train a Neural Network Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dxNtnXxZz8P1"
   },
   "source": [
    "Train a neural network classifier (`MLPClassifier` from `sklearn.neural_network`) using the following settings: `hidden_layer_sizes=(10), solver='adam', n_iter_no_change=100, max_iter=10000, alpha=0`. Compute the accuracy on the training and test data. You should oberve much worse performance than with logistic regression before. What might be the reason?\n",
    "\n",
    "**Hint**: the usage of `MLPClassifier` can be referred from [here.](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "wingtJ-0ZSJY"
   },
   "outputs": [],
   "source": [
    "#@title Double click to show hint code\n",
    "\n",
    "clf = ... # Instantiate a multilayer perceptron (MLP) classifier with above mentioned parameters\n",
    "... # Train the MLP classifier using the training dataset\n",
    "y_predict = ... # Make predictions for the training & testing dataset\n",
    "acc = ... # Use propoer scoring function to compute the accuracy of your predictions\n",
    "print(\"Accuracy on test data: %.2f %%\" % acc_train)\n",
    "print(\"Accuracy on test data: %.2f %%\" % acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "M4JoIktuz8P1"
   },
   "outputs": [],
   "source": [
    "#@title Double click to show solution\n",
    "\n",
    "import sklearn.neural_network\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf =  MLPClassifier(hidden_layer_sizes=(10), solver='adam', n_iter_no_change=100, max_iter=10000, alpha=0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_train = clf.predict(X_train)\n",
    "acc_train = sklearn.metrics.accuracy_score(y_train, y_pred_train)\n",
    "print(\"Accuracy on training data: %.2f %%\" % acc_train)\n",
    "y_pred_test = clf.predict(X_test)\n",
    "acc_test = sklearn.metrics.accuracy_score(y_test, y_pred_test)\n",
    "print(\"Accuracy on test data: %.2f %%\" % acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "neural.network package includes the Classifier with many opptions - hidden layer, how solved, how big,... \n",
    "can fit it, predict accuracies, this was applied to unclassified data, hence only 60% performance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RMm2dU0Zz8P3"
   },
   "source": [
    "Normalize the data before learning the MLP Classifier. You can use `MinMaxScaler` from `sklearn.preprocessing` for that purpose. What performance does your MLP classifier achieve now? How did the feature statistics change by applyting the `MinMaxScaler`?\n",
    "\n",
    "**Hint**: the usage of `MinMaxScaler` can be referred from [here.](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "5R8LbBDOaTw_"
   },
   "outputs": [],
   "source": [
    "#@title Double click to show hint code\n",
    "\n",
    "# Step 1: Normalize the training data\n",
    "# Step 2: Instantiate a new MLP classifier\n",
    "# Step 3: Train the new MLP classifier with normalized training data\n",
    "# Step 4: Use the same normalization settting to transform test data\n",
    "# Step 5: Evaludate the new classifier with normalized training and test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "ftyCEpDQz8P4"
   },
   "outputs": [],
   "source": [
    "#@title Double click to show solution\n",
    "import sklearn.neural_network\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "\n",
    "clf =  MLPClassifier(hidden_layer_sizes=(10), solver='adam', n_iter_no_change=100, max_iter=10000, alpha=0)\n",
    "clf.fit(scaler.transform(X_train), y_train)\n",
    "y_pred_train = clf.predict(scaler.transform(X_train))\n",
    "acc_train = sklearn.metrics.accuracy_score(y_train, y_pred_train)\n",
    "print(\"Accuracy on training data: %.2f %%\" % acc_train)\n",
    "y_pred_test = clf.predict(scaler.transform(X_test))\n",
    "acc_test = sklearn.metrics.accuracy_score(y_test, y_pred_test)\n",
    "print(\"Accuracy on test data: %.2f %%\" % acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here normalized data to get better performance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xO_JmKGJz8P6"
   },
   "source": [
    "## Selecting a Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gIPVMEvQz8P7"
   },
   "source": [
    "We want to pick the best classifier among Logistic Regression, MLP Classifier and a decision tree classifier (`sklearn.tree.DecisionTreeClassifier`). We therefore need to learn these models and evaluate. Run this experiment on the already chosen split of training and testing data. What result do you get? Which model works best? Is this consistent on the training and test data?\n",
    "\n",
    "**Hint**: for the usage of decision tree classifier, please refer to [here.](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "guAmR9TKcvPl"
   },
   "outputs": [],
   "source": [
    "#@title Double click to show hint code\n",
    "\n",
    "# Step 1: Define a list of instantiated classifiers, \n",
    "#         i.e., LR, MLP and decision tree.\n",
    "# Step 2: Train each classifier with training data and \n",
    "#         evaluate with both training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "rQAGK0lIz8P7"
   },
   "outputs": [],
   "source": [
    "#@title Double click to show solution\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "classifiers = {'Logistic Regression': sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=1000),\n",
    "               'MLP Classifier': MLPClassifier(hidden_layer_sizes=(10), solver='adam', n_iter_no_change=100, max_iter=10000, alpha=0),\n",
    "               'Decision Tree': DecisionTreeClassifier()}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    print(\"MODEL: %s\" % name)\n",
    "    clf.fit(scaler.transform(X_train), y_train)\n",
    "    y_pred_train = clf.predict(scaler.transform(X_train))\n",
    "    acc_train = sklearn.metrics.accuracy_score(y_train, y_pred_train)\n",
    "    print(\"Accuracy on training data: %.2f %%\" % acc_train)\n",
    "    y_pred_test = clf.predict(scaler.transform(X_test))\n",
    "    acc_test = sklearn.metrics.accuracy_score(y_test, y_pred_test)\n",
    "    print(\"Accuracy on test data: %.2f %%\" % acc_test)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pick among 3 to see which one ist best - repeat and do the same with cross validation \n",
    "different performance for each of the folds- shows that the test sets and predictions are random \n",
    "need to be careful with evaluation - cross-validation allows you to get rid of some randomness "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0mYgSAdOz8P9"
   },
   "source": [
    "Repeat the above experiment but don't use a single training and test set but rather use 10-fold cross-validation (`sklearn.model_selection.cross_val_score`). How stable are the accuracies for all of the models? If you consider the mean of the cross-validation accuracies for each of the model, which model performs best?\n",
    "\n",
    "**Hint**: for the usage of cross validation and scoring function, please refer to [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "tI6S-lVRel0R"
   },
   "outputs": [],
   "source": [
    "#@title Double click to show hint code\n",
    "\n",
    "# Step 1: Instantiate a model using `cross_val_score`\n",
    "# Step 2: Read the suggested doc and use it to compute \n",
    "#         the accuracy for each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "m9frslWfz8P-"
   },
   "outputs": [],
   "source": [
    "#@title Double click to show solution\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    print(\"MODEL: %s\" % name)\n",
    "    acc = cross_val_score(clf, data.data, data.target, cv=10)\n",
    "    print(\"Scores: \", acc)\n",
    "    print(\"Mean acc: %.4f +/- %.4f\" % (np.mean(acc), np.std(acc)))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SXj8DCIzz8QA"
   },
   "source": [
    "# Underfitting/Overfitting and Model Selection on a Regression Task (inspired by the sklearn docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WXnCpy6Iz8QB"
   },
   "source": [
    "To study overfitting we create an artifical dataset for regression. The true data follows a sinusoid function, but our training data is corrupted with noise. Furthermore we have a test set on which we want to achieve good regression performance and, therfore, need to select a good model for regression. To begin, execute the next block of code for generating the training and the testing data. The data will also be visualized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "r4tsCYWsz8QB"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn.neural_network\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# generate the data\n",
    "def true_fun(X):\n",
    "    return np.sin(8*X)\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "n_samples = 500\n",
    "X = np.sort(np.random.rand(n_samples))\n",
    "y = true_fun(X) + np.random.randn(n_samples) * 0.1\n",
    "\n",
    "# select a subset of the data for training and testing\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.95, train_size=None)\n",
    "\n",
    "# for better visualization, sort test data\n",
    "order = np.argsort(X_test)\n",
    "X_test = X_test[order]\n",
    "y_test = y_test[order]\n",
    "\n",
    "# visualize data\n",
    "plt.plot(X_test, true_fun(X_test), 'g-', label=\"True function\")\n",
    "plt.scatter(X_train, y_train, edgecolor='b', s=20, label=\"Training Samples\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.xlim((0, 1))\n",
    "plt.ylim((-2, 2))\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use cross validation to automatically select a good model order (features of order 6 best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IPhSlEUiz8QD"
   },
   "source": [
    "We now want to train linear regressors based on different features constructed from the (first order features, second order features, etc.). With first order features, we clearly can't fit the function perfectly, with higher order features we will fit the data better but also risk to overfit to the training samples. Use `PolynomialFeatures` and `LinearRegression` as well as `Pipeline` to train linear regressors using polynomial features. Visually identify the degree for which there seems to be a good fit of the data (plot your data fit). For which orders of the features do we observe underfitting, for which overfitting? What is the sweetspot?\n",
    "\n",
    "\n",
    "**Hint**:\n",
    "\n",
    "\n",
    "1. For the usage `PolynomialFeatures`, please refer to [here.](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)\n",
    "2. `Pipeline` is one of the nicest feature of `sklearn`, which can significantly lessen the effort in writing a 'sklearn-style' classificaiton algorithm. An easy user guide can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) and for complete usage guidance please refer to [here.](https://scikit-learn.org/stable/modules/compose.html#pipeline)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "KOItITZShiIp"
   },
   "outputs": [],
   "source": [
    "#@title Double click to show hint code\n",
    "\n",
    "# Step 1: Choose one degree for polynomial model fitting.\n",
    "# Step 2: Instantiate the model of PolynomialFeatures and LinearRegression\n",
    "# Step 3: Instantiate a Pipeline model based on the two instances \n",
    "#         generated from Step 2.\n",
    "# Step 4: Fit the Pipeline model with training data\n",
    "# Step 5: Compute the predicted value based on testing data\n",
    "# Step 6: Using following snippets to plot the true function \n",
    "#         and fitted function\n",
    "\n",
    "\"\"\"\n",
    "plt.plot(test_data, predicted_value, label=\"Model\")\n",
    "plt.plot(test_data, true_fun(test_data), label=\"True function\")\n",
    "plt.scatter(X, y, edgecolor='b', s=20, label=\"Samples\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.xlim((0, 1))\n",
    "plt.ylim((-2, 2))\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"Degree {}\".format(degrees selected from step 1))\n",
    "\n",
    "\"\"\"\n",
    "# Step 7: Choose another degree and repeat from step 2\n",
    "#         until completing all desired degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "iDaa_g4-z8QE"
   },
   "outputs": [],
   "source": [
    "#@title Double click to show solution\n",
    "\n",
    "degrees = [1,5,10,20]\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "for i in range(len(degrees)):\n",
    "    ax = plt.subplot(1, len(degrees), i + 1)\n",
    "    plt.setp(ax, xticks=(), yticks=())\n",
    "\n",
    "    polynomial_features = PolynomialFeatures(degree=degrees[i],\n",
    "                                             include_bias=False)\n",
    "    linear_regression = LinearRegression()\n",
    "    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                         (\"linear_regression\", linear_regression)])\n",
    "    pipeline.fit(X_train[:,np.newaxis], y_train)\n",
    "\n",
    "    plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=\"Model\")\n",
    "    plt.plot(X_test, true_fun(X_test), label=\"True function\")\n",
    "    plt.scatter(X, y, edgecolor='b', s=20, label=\"Samples\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.xlim((0, 1))\n",
    "    plt.ylim((-2, 2))\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"Degree {}\".format(\n",
    "        degrees[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QtjwW5Sez8QG"
   },
   "source": [
    "Using cross-validation, compute scores for each order of the polynomial features. Which order works best? Does it match your result from the previous task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "IHvXLzxEkPWd"
   },
   "outputs": [],
   "source": [
    "#@title Double click to show hint code\n",
    "\n",
    "# Step 1: Choose one degree for polynomial model from a list of degrees\n",
    "# Step 2: Instantiate the Pipeline model based on your answer from last question\n",
    "# Step 3: Use cross_val_score to evaluate the Pipeline instance \n",
    "#         using scoring metric as 'neg_mean_squared_error'\n",
    "# Step 4: Append the obtained score into a list of score which is\n",
    "#         with the same size of the list of degree\n",
    "# Step 5: Find the degree associated with best score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "bfF4BOGSz8QG"
   },
   "outputs": [],
   "source": [
    "#@title Double click to show solution\n",
    "\n",
    "degrees = range(1,21)\n",
    "\n",
    "all_scores = []\n",
    "for i in range(len(degrees)):\n",
    "    polynomial_features = PolynomialFeatures(degree=degrees[i],\n",
    "                                             include_bias=False)\n",
    "    linear_regression = LinearRegression()\n",
    "    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                         (\"linear_regression\", linear_regression)])\n",
    "    #pipeline.fit(X_train[:,np.newaxis], y_train)\n",
    "    scores = -cross_val_score(pipeline, X[:, np.newaxis], y,\n",
    "                             scoring=\"neg_mean_squared_error\", cv=10)\n",
    "    all_scores.append(np.mean(scores))\n",
    "\n",
    "best_idx = np.argmin(all_scores)\n",
    "print(\"The best performance is achieved for %d-th order features.\" % degrees[best_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nGZ_o4-4z8QI"
   },
   "source": [
    "# Bonus: The Best Place for Sunbathing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ScI1f5lNz8QI"
   },
   "source": [
    "Assume there was no Corona virus outbreak and we would all be meeting in Austria. Naturally we would be wondering where the best place for sunbathing is (let's assume the best place is the place with the highest temperature). Therefore, I have prepared weather information from a few locations in Austria. Your goal is to learn a regression model that predicts the temperature all over Austria from temperature readings at fixed sensor locations and identify the location with highest temperature.\n",
    "\n",
    "The data provided to you is temperature information of past 9 days (Sep.3rd.2020 - Sep.11th.2020) in the following form: \n",
    " \n",
    "\n",
    "1. Files with keyword 'sensor' represent the temperature at 50 locations randomly selected from within Austria and its surrounding.\n",
    "2. Files with keyword 'full' represent the temperature information at 800 locations from within Austria and its surrounding (50 of these correspond to locations for which you know the temperature).\n",
    "3. Files with suffix 'train' include the temperature informaition from Sep.3rd. to Sep.10th and files with suffix 'test' contain the temperature of Sep. 11th.  \n",
    "\n",
    "Train a regression model to predict the temperate at all these 800 locations using the data from the 8 given days. You are then given a test example, also including the temperature at the 50 locations and you are supposed to find the place with the highest temperature. Compare your result to the ground truth temperatures of the test example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "colab_type": "code",
    "id": "XdZTPUW2z8QJ",
    "outputId": "69b70643-d899-431f-9055-ce9f45fcd474"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 226.772x226.772 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "y = np.load('2020-08-06-full.npy')\n",
    "y = y.squeeze()\n",
    "x = np.load('2020-08-06-sensor.npy')\n",
    "coordinates_lat, coordinates_lon = np.load('coordinates.npy')\n",
    "\n",
    "def plot_weather(data, coordinates_lat, coordinates_lon):\n",
    "    \"\"\"\n",
    "    Creating a plot of data in the shape of Austria. Data is a matrix of size 20x40 corresponding\n",
    "    to the 800 locations.\n",
    "    \"\"\"\n",
    "    mask = np.load(\"mask.npy\")\n",
    "\n",
    "    pdata = np.multiply(mask, data)\n",
    "    \n",
    "    # plot\n",
    "    plt.figure(figsize=(8 / 2.54, 8 / 2.54))\n",
    "    plt.xlabel(\"longitude\")\n",
    "    plt.ylabel(\"latitude\")\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(pdata[::-1])\n",
    "    cbar = plt.colorbar()\n",
    "    cbar.set_label('temperature [celsius]')\n",
    "    plt.show()\n",
    "            \n",
    "    \n",
    "plot_weather(y, coordinates_lat, coordinates_lon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fills in the temperatures when given some measurement poinst- predict where the temperature is highest \n",
    "\n",
    "multiple ways: \n",
    "could learn linear regression models as input temperature information and predict temperature everywere \n",
    "train on traing data and make prediction and get one max (optimum) BUT problem not direct spatial information hence one could think about other types of models for local neighbourhoods such as neural networks but not necessary "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PRNI_ML_Tutorial.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
