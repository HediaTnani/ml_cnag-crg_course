# -*- coding: utf-8 -*-
"""ML_course_CNAG-CRG_day2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1I2nf-XxQKdv5DfOw8S1rXg3vwtvGyz75

# Supervised learning

Supervised learning aims to predict labels of data using annotated examples. Supervised algorithms learn patterns on "tagged" data for later predicting the outcome of data which the model has not seen before. 

There are two branches in supervised learning:


*   **Classification:** learns and predicts discrete labels on the data (e.g. Diabetic/Healthy, Dead cell/Live cell...)  
*   **Regression:** predicts a continuous value based on the a set of explanatory variables (e.g. weight based on the height or age based on methylation pattern).

Two important concepts in supervised learning are **response or dependent variable** and the **predictive or independent variables**. Supervised models aim to predict the response variable using the values from the predictor variables.

Let's look at examples:

The iris dataset contains information about 3 species of plants.
"""

# import packages
import numpy as np
import pandas as pd
from sklearn import datasets

# load dataset
iris_dataset = datasets.load_iris()

# create data frame with predictive and variables
iris_table = pd.DataFrame(data = iris_dataset['data'], 
                          columns = iris_dataset['feature_names'])

# add response variable to the table
iris_table['species'] = pd.Categorical.from_codes(iris_dataset.target, iris_dataset.target_names)

# show first five rows of the dataframe
iris_table.head()

"""Another example is the diabetes dataset."""

# load dataset
diabetes_dataset = datasets.load_diabetes()

# create data frame with predictive variables
diabetes_table = pd.DataFrame(data = diabetes_dataset['data'],
                              columns = diabetes_dataset['feature_names'])

# add response variable to the table
diabetes_table['disease_progression'] = pd.Series(data = diabetes_dataset['target'])

# show first five rows of the dataframe
diabetes_table.head()

"""# Regression

Regression algorithms learn the relationship between the predictor and the response variable. The output is a continuous value.

## Simple linear regression

Simple linear regression tries to find the linear relationship between one predictor variable and the response variable. In other words it tries to find the line which best fits the data. It follows the formula:

<br>

 $$ 
  \hat y = \beta_0 + \beta_1  x 
 $$

<br>

* $\hat y$ = predicted value
* $x$ = predictor variable
* $\beta_1$ = model coefficient (slope of the line in simple linear regression)
* $\beta_0$ = bias term (intercept in simple linear regression)

Let's see a simple example with the iris dataset:
"""

# Commented out IPython magic to ensure Python compatibility.
import seaborn as sns
# %matplotlib inline

# plot the petal width v.s. the petal length
sns.relplot(data = iris_table,
            x = iris_table.columns[2], y = iris_table.columns[3])

"""<br>

We can now fit a model which predicts the petal width using information about the petal length. We will use the package **scikit-learn**. We first need to put the data in the correct format to run scikit learn models on it. 
"""

# prepare data

# create new arrays for the predictive and response variables 
# It is important to add an additional dimension to our numpy array to fit the model
pet_wid = iris_dataset['data'][:, np.newaxis, 3]
pet_len = iris_dataset['data'][:, np.newaxis, 2]

pet_len[1:10]

from sklearn import linear_model

# create a linear regression object
regres_iris = linear_model.LinearRegression()

# fit linear regression model to the data
regres_iris.fit(pet_len, pet_wid)

# extract slope 
m = round(float(regres_iris.coef_), 3)

# extract intercept
n = round(float(regres_iris.intercept_), 3)

print('slope = {} and intercept = {}'.format(m, n))

"""<br>
Plot fitted line
"""

import matplotlib.pyplot as plt

plt.plot(pet_len, pet_wid, 'o')
plt.plot(pet_len, m*pet_len + n)
plt.xlabel("petal length (cm)")
plt.ylabel("petal width (cm)")

"""<br>

The linear model follows the equation shown above:

<br>

 $$ 
  \hat y = \beta_0 + \beta_1  x 
 $$

<br>

* $ \beta_1 $ = 0.42
* $ \beta_0 $ = -0.36

<br>

$$ \hat y = 0.42 -0.36 x $$

<br>

How are $ \beta_0 $ and $ \beta_1 $ estimated? In other words how is the line which fits the data best found?

Regression algorithms aim to find the model coefficients which minimise the loss function. Choosing the apropriate loss function is key in machine learning. In the case of simple linear regression the aim is to find the line which minimises the squared difference between the predicted values and the true values. 

Let's look at it in this image: 

<br>

![image.png](https://littleml.files.wordpress.com/2019/03/residuals-1.png)

<br>

The difference between the predicted value and the true value is called residual (doted lines). To account for the fact that predictions can be higher and lower than the true value (this is residuals can be positive or negative), a linear model aims to minimise the Residual Sum of Squares (RSS). This is also called least squares estimation:

<br>

$$ RSS(\beta) = \sum_{i=1}^{N} (\hat y - y)^2 $$

<br>

* $\beta$ = {$\beta_0, \beta_1$... $\beta_n$}
* $N$ = number of data points
* $\hat y$ = predicted value
* $y$ = true value

<br>


For linear regression models, there is a closed-formed solution to compute the optimal coefficient values using the [Normal equation](https://towardsdatascience.com/performing-linear-regression-using-the-normal-equation-6372ed3c57).

### Metrics to evaluate the models

There are different metrics to evaluate the performance of linear regression models.

#### Mean Absolute Error (MAE)

The **Mean Absolute Error** is perhaps the simplest metric. It computes the average absolute difference between predictions and actual values. It therefore follows the formula:

$$ MAE(\beta) = {\frac{1}{N}}\sum_{i=1}^{N} |\hat y - y| $$ 

* $\beta$ = {$\beta_0, \beta_1$... $\beta_n$}
* $N$ = number of data points
* $\hat y$ = predicted value
* $y$ = true value


<br>


In the case of MAE all residuals are weighted equally. For this reason it is not optimal when outliers are present.

The interpretation of this metric relatively straight forward. It indicates the average error of the model (regardless of direction). For example, if a linear regression model which predicts the height (cm) based on the weight (kg) has MAE = 3, it indicates that the model predictions have an average deviation of Â±3 cm from the true values.

<br>

#### Root Mean Square Error (RMSE)

The **Root Mean Square Error**  is another common metric to evaluate the performance of machine learning algorithms. It follows the formula:

<br>

$$ RMSE(\beta) =  \sqrt {\frac{1}{N}\sum_{i=1}^{N}(\hat y - y)^2} $$


* $\beta$ = {$\beta_0, \beta_1$... $\beta_n$}
* $N$ = number of data points
* $\hat y$ = predicted value
* $y$ = true value

<br>

Since the differences between predicted and true values are squared, the importance of errors increases quadratically with their value. This means that predictions which are very far from the true values are penalised by RMSE.

#### Coeficient of determination ($ R^2$)

There are other metrics which evaluate how good the model fits the data. The coefficient of determination or R-squared ($R^2$) is one example. It measures the proportion of total variance of the response variable explained by model. The higher the value the better the model fits the data. It follows the equation:

<br>

$$ R^2 = 1 - \frac {SS_{res}}{SS_{tot}} = 1 - \frac {\sum_{i=1}^{N} (\hat y - y)^2}{\sum_{i=1}^{N}(y - \bar y)^2} $$

* $ \hat y $ =  predicted value
* $ y $ = true value
* $ \bar y $ = mean of response variable
* $ N $ = number of data points

<br>

### Build linear model

Let's now fit a simple linear model to the iris dataset and make predictions. In this case we are interested in predicting the petal width (response variable) using the petal length as predictor variable.


The process has the following steps:

1. Split dataset into training and test set
2. Fit linear model to the training data
3. Apply the model to test set.
4. Evaluate the model performance (MAE, RMSE and $R^2$)

#### Prepare data
"""

# split dataset into training and test set

# set a random seed so that the randomly sampled rows are always the same
import random
random.seed(20)

# sample 20 row numbers to use as test set
rows_test = random.sample(range(0,pet_len.shape[0]),20)

# split predictor variable into test and training set
pet_len_test = pet_len[rows_test]
pet_len_train = np.delete(pet_len, rows_test)[:,np.newaxis]


# split response variable into test and training set
pet_wid_test = pet_wid[rows_test]
pet_wid_train = np.delete(pet_wid, rows_test)[:, np.newaxis]


print("Training set has {} observations and test set has {}".format(pet_len_train.shape[0], pet_len_test.shape[0]))

"""#### Fit model to training data"""

# fit a simple linear regression model to the training set

# create a linear regression object
regres_pet = linear_model.LinearRegression()

# fit a linear model to the training data
regres_pet.fit(pet_len_train, pet_wid_train)

# plot petal length v.s. width and the fitted line to the training set
plt.plot(pet_len_train , pet_wid_train, 'o')
plt.plot(pet_len_train, regres_pet.predict(pet_len_train))
plt.xlabel("petal length (cm)")
plt.ylabel("petal width (cm)")

"""#### Predict new values"""

# make predictions on the test set
pet_wid_pred = regres_pet.predict(pet_len_test)

# create data frame with predicted and true values
pred_table_wid = pd.DataFrame({'Predicted_width': pet_wid_pred.flatten(), 'True_width': pet_wid_test.flatten()})

# print data frame 
pred_table_wid

# Make scatter plot of predicted vs true petal width
plt.plot(pet_wid_test, pet_wid_pred, 'o')
plt.plot(pet_wid_test, pet_wid_test)
plt.xlabel('True petal width (cm)')
plt.ylabel('Predicted petal width (cm)')

# make residuals plot
sns.residplot(pet_wid_test, pet_wid_pred)
plt.xlabel("True petal width (cm)")
plt.ylabel("Residuals")

"""#### Evaluate the model"""

# evaluate the performance of the model

# import metrics package
from sklearn import metrics

# compute MAE
petal_mae = metrics.mean_absolute_error(pet_wid_pred, pet_wid_test)

# get RMSE
petal_rmse = metrics.mean_squared_error(pet_wid_pred, pet_wid_test,
                           squared = False)

print("The model has a MAE = {} (cm) and a RMSE = {} (cm)".format(round(petal_mae, 3), round(petal_rmse, 3)))

# get squared R 
metrics.r2_score(pet_wid_pred, pet_wid_test)

"""### Exercise 

Let's now do an exercise. The iris dataset contains two additional variables sepal length and sepal width. How well can we predict the length of sepals using the length of petals? 

1. Fit a linear model to predict sepal length uisng the petal length 
2. Predict the sepal length of a test set using the model
3. Compute the MAE, RMSE and $R^2$.

"""

# the column we are interested now are 0 and 2
iris_table

# the relationship seems different for the setosa species
sns.relplot(data = iris_table, x = iris_table.columns[0], y = iris_table.columns[2], hue="species")

# exclude setosa species for the downstream analysis

# get sepal length for versicolor and virginica species
sep_len = iris_table[iris_table.species != "setosa"].iloc[:, 0][:, np.newaxis]

# get petal length for versicolor and virginica species
pet_len_filt = iris_table[iris_table.species != "setosa"].iloc[:,2][:, np.newaxis]

"""#### Solution"""

random.seed(10)

# sample 5 row numbers to use as test set
rows_test = random.sample(range(0, sep_len.shape[0]),20)

# split predictor variable into test and training set
sep_len_train = np.delete(sep_len, rows_test)[:,np.newaxis]
sep_len_test = sep_len[rows_test]

# split response variable into test and training set 
pet_len_train = np.delete(pet_len_filt, rows_test)[:, np.newaxis]
pet_len_test = pet_len_filt[rows_test]

# create a linear regression object
regres_sep = linear_model.LinearRegression()

# fit a linear model to the training data
regres_sep.fit(pet_len_train, sep_len_train)

# predict values of test set
sep_len_pred = regres_sep.predict(pet_len_test)

# plot predicted values v.s. true values
plt.plot(sep_len_test, sep_len_pred, 'o')
plt.plot(sep_len_test, sep_len_test, "-.")
plt.xlabel("True sepal width")
plt.ylabel("Predicted sepal length")

# plot fitted line to training data
plt.plot(pet_len_train, sep_len_train, 'o')
plt.plot(pet_len_train, regres_sep.predict(pet_len_train))

# get MAE 
metrics.mean_absolute_error(sep_len_test, sep_len_pred)

# get RMSE 
metrics.mean_squared_error(sep_len_test, sep_len_pred, squared = False)

# get squared R
metrics.r2_score(sep_len_test, sep_len_pred)

"""## Multiple linear regression

In many cases we have more than one predictor variable describing the response variable. The same linear regression model can be applied in such cases. In that case there is one additional parameter per additional predictor variable. With these multidimensional data instead of finding the line which best fits the data, the plane (2 predictor variables) or hyperplane (more than 2 predictor variables) is sought.

<br>

$$ \hat y = \beta_0 + \beta_1x_{1} + \beta_2x_{2} ... \beta_nx_{n} $$ 

<br>

* $\hat y $ = predicted value 
* $ \beta_i $ = parameters or coefficients of the model
* $ x_{i} $ = predictor variable  

<br> 

Let's fit a multiple linear regression model to the diabetes dataset. The response variable in this case is termed disase progression.

### Explore dataset

A good idea when prompted with a new dataset is to explore the different independent variables and the response variable. In this case the independent variable is **disase progression**.
"""

# make histogram of disease progression values
sns.histplot(data = diabetes_table, x = 'disease_progression')

"""In order to explore the predictive variables the pariplot from the seaborn package is a very handy tool to look at the distribution of values in each variable as well as the possible relationships between them. """

# Since there are a few dependent variables we will only look at a subset of them
sns.pairplot(diabetes_table[['age', 'bmi', 's2', 's3', 'disease_progression']])

"""### Train model"""

# split dataset into test and training set
from sklearn.model_selection import train_test_split

# scikit learn includes a function to do so
# we use 80% of the data for training and 20 % as test set
diab_train, diab_test, progr_train, progr_test = train_test_split(diabetes_dataset['data'], diabetes_dataset['target'],
                                                                  test_size = 0.2, random_state = 42)


print("Training set has {} observations and test set has {}".format(diab_train.shape[0], diab_test.shape[0]))

# create a linear regression object
diab_lin_model = linear_model.LinearRegression()

# fit the linear model to the training data
diab_lin_model = diab_lin_model.fit(diab_train, progr_train)

# predict values of test set
diab_pred = diab_lin_model.predict(diab_test)

# make data frame with predicted and true values
pred_table_diab = pd.DataFrame.from_dict({'Predicted_progression': diab_pred.flatten(), 'True_progression': progr_test.flatten()})

# print data frame 
pred_table_diab

# plot predicted values vs real values
plt.plot(progr_test, diab_pred, 'o')
plt.plot(progr_test, progr_test, "--")
plt.xlabel("True progression (a.u.)")
plt.ylabel("Predicted progression(a.u.)")

# get MAE
diab_mae = metrics.mean_absolute_error(progr_test, diab_pred)

print('The MAE of the model is {}'.format(diab_mae))

# get RMSE
diab_rmse = metrics.mean_squared_error(progr_test, diab_pred,
                                       squared = False)

print('The RMSE of the model is {}'.format(diab_rmse))

# get squared R 
diab_rsqr = metrics.r2_score(progr_test, diab_pred)

print('The R^2 of the model is {}'.format(diab_rsqr))

"""### Cross-validation

The performance metrics of the model can be influenced by the arbitrary partition of the dataset into training and test. That is why cross-validation gives more accurate performance metrics of regression algorithms.

### Exercise
To end this section let's explore the relationship between methylation and age. In the dataset below we can find the methylation levels for 53 CpGs from 100 individuals as well as the corresponding age. The dataset was obtained from [Daunay et al., 2019](https://www.nature.com/articles/s41598-019-45197-w#Sec14). Methylation levels were measured from blood samples using pyrosequencing. 

**Task:** Can we predict the age of the individuals based on the methylation levels using linear regression? How accurate are our predictions? 

**Bonus exercise:** Train a multiple linear regression model on only the 1st 23 methylation sites and compare the performance to the model trained on the complete dataset. Discuss the obtained results.
"""

# load dataset 
age_table = pd.read_csv("https://raw.githubusercontent.com/IvoLeist/ml_cnag-crg_course/main/day2/methylation_dataset.csv")

age_table

"""#### Solution"""

# make histogram of age 
sns.histplot(age_table['age'])

# explore a few of the methylation sites
sns.pairplot(age_table.iloc[:,[0,1,2,3]])

# transform it into an array
meth_array = age_table.to_numpy()

# split dataset into test and training set
meth_train, meth_test, age_train, age_test = train_test_split(meth_array[:,1:99], meth_array[:, 0],
                                                              test_size = 0.2, random_state = 42)

# create a linear regression object
age_lin_model = linear_model.LinearRegression()

# fit the linear model to the training data
age_lin_model = age_lin_model.fit(meth_train, age_train)

# predict values of test set
age_pred = age_lin_model.predict(meth_test)

# make data frame with predicted and true values
pred_table_age = pd.DataFrame.from_dict({'Predicted_age': age_pred.flatten(), 'Chronological_age': age_test.flatten()})

# print data frame 
pred_table_age

# plot predicted age v.s. chronological age 
plt.plot(age_test ,age_pred, 'o')
plt.plot(age_test, age_test)
plt.xlabel("Chronological age (years)")
plt.ylabel("Predicted age (years)")

# compute MAE 
age_mae = metrics.mean_absolute_error(age_test, age_pred)

# compute RMSE 
age_rmse = metrics.mean_squared_error(age_test, age_pred, squared = False)

# compute squared R
age_sqrtr = metrics.r2_score(age_test, age_pred)

print("The model predicts age with accuracy Â±{} years".format(round(age_mae, 2)))

"""##### Solution bonus

Let's look at the model performance when we select only a subset of methylation sites (first 23 sites).
"""

# subset the dataframe to take only the 1st 23 sites
age_table_filt = age_table.iloc[:,0:23]

# convert the dataframe to array
meth_array_filt = age_table_filt.to_numpy()

# split dataset into test and training set
meth_filt_train, meth_filt_test, age_filt_train, age_filt_test = train_test_split(meth_array_filt[:,1:99], meth_array_filt[:,0],
                                                                                  test_size = 0.2, random_state = 42)

# create a linear regression object
age_filt_model = linear_model.Lasso()

# fit the linear model to the training data
age_filt_model = age_filt_model.fit(meth_filt_train, age_filt_train)

# predict values of test set
age_pred_filt = age_filt_model.predict(meth_filt_test)

# make data frame with predicted and true values
pred_filt_age = pd.DataFrame.from_dict({'Predicted_age': age_pred_filt.flatten(), 'Chronological_age': age_filt_test.flatten()})

# print data frame 
pred_filt_age

# compute MAE 
mae_filt = metrics.mean_absolute_error(age_filt_test, age_pred_filt)

#compute RMSE
rmse_filt = metrics.mean_squared_error(age_filt_test, age_pred_filt, squared = False)

# compute R-squared
sqrt_filt = metrics.r2_score(age_filt_test, age_pred_filt)

"""Now we can compare the performance of both models looking at MAE, RMSE and R-squared."""

# MAE

print("MAE of model with complete dataset: {}".format(age_mae))
print("MAE of model with subseted dataset: {}".format(mae_filt))

# RMSE

print("RMSE of model with complete dataset: {}".format(age_rmse))
print("RMSE of model with subseted dataset: {}".format((rmse_filt)))

# Coefficient of determination

print("R-squared of model with complete dataset: {}".format(age_sqrtr))
print("R-squared of model with subseted dataset: {}".format((sqrt_filt)))

# let's explore the sites which we filtered out
sns.pairplot(age_table.iloc[:,[0, 30, 31,32,33]])

"""## Problems of linear regression

### Non-linearity of the data

Linear regression models assume that there is a linear relationship between predictor and response variables. When this is not the case linear models can perform poorly. Let's see an example:
"""

# load boston dataset
boston_dataset = datasets.load_boston()

# get dependent and independent variables as arrays
y_nox = boston_dataset['data'][:, np.newaxis ,4]
x_dis = boston_dataset['data'][:, np.newaxis ,7]

# make dataframe with dependent and independent variable
boston_table = pd.DataFrame.from_dict({'y_nox': y_nox.flatten(),
                                       'x_dis': x_dis.flatten()})

# make scatter plot
sns.relplot(data = boston_table, x = boston_table.columns[1], y = boston_table.columns[0])

# make residuals plot
sns.residplot(x = x_dis.flatten(), y = y_nox.flatten())

"""## Polynomial regression

There are occasions in which the relationship between the dependent and independent variables is not linear. In such cases a linear regression model would not fit the data well. 

Let's see an example:
"""

bost_lin_model = linear_model.LinearRegression().fit(x_dis, y_nox)

# compute squared R
rsqr_bos_lin = metrics.r2_score(y_nox, bost_lin_model.predict(x_dis))

print('The R^2 of the model is {}'.format(rsqr_bos_lin))

# get RMSE
rmse_bos_lin = metrics.mean_squared_error(y_nox, bost_lin_model.predict(x_dis),
                                         squared = False)

print('The RMSE of the model is {}'.format(rmse_bos_lin))

# make scatter plot with fitted line
plt.plot(x_dis, y_nox, 'o')
plt.plot(x_dis, bost_lin_model.predict(x_dis), 'r')

"""The following steps are done when carrying out polynomial regression:

1. Generate polynomial features from original features.
2. Fit a linear regression model to the extended feature set.
3. Evalute the fit of the model

<br>
"""

from sklearn.preprocessing import PolynomialFeatures

# generate new features (second degree polynomial)
poly_features = PolynomialFeatures(degree = 2, include_bias = False)
x_dis_poly = poly_features.fit_transform(x_dis)

x_dis_poly[0]

# fit linear model to extended data
bost_poly_model = linear_model.LinearRegression().fit(x_dis_poly, y_nox)

# compute squared R
rsqr_bos_poly = metrics.r2_score(y_nox, bost_poly_model.predict(x_dis_poly))

print('The R^2 of the model is {}'.format(rsqr_bos_poly))

# get RMSE
rmse_bos_poly = metrics.mean_squared_error(y_nox, bost_poly_model.predict(x_dis_poly),
                                         squared = False)

print('The RMSE of the model is {}'.format(rmse_bos_poly))

# make scatter plot with predicted and real values
plt.plot(x_dis, y_nox, 'o')
plt.plot(x_dis, bost_poly_model.predict(x_dis_poly), 'o')

# get coefficients from the model 
bost_poly_model.coef_

"""Since we used a polynomial of degree 2, out model follows the equation:

<br>

$$ \hat y = \beta_0 + \beta_1x_1 + \beta_2x_1^2  $$

# Classification

## Decission Tree

A decision tree is a supervised learning algorithm that is perfect for classification problems, as itâs able to order classes on a precise level. It works like a flow chart, separating data points into two similar categories at a time from the âtree trunkâ to âbranches,â to âleaves,â where the categories become more finitely similar. This creates categories within categories, allowing for organic classification with limited human supervision.

schematic           |  example
:-------------------------:|:-------------------------:
![image.png](https://miro.medium.com/max/700/1*SKFU4V4qWNDU8sXgq36ncw.png)|![image.png](https://d33wubrfki0l68.cloudfront.net/cb281b80c41c9e76eb327e26ed5e0e6e5f05fc7f/31de9/static/b42ef5448b11ec2f2ec20ca7f97cbb3c/9cda9/decision-tree-sports.png)

**Underlying principle of the algorithm**

It recursively splits the training set till it no longer
finds a split which reduces the impurity

first split|count missclassification
:-------------------------:|:-------------------------:
![image.png](https://miro.medium.com/max/501/0*UvBgln5Di3yeLZHI.png)|![image.png](https://miro.medium.com/max/500/0*NFb_YzdigP99hgjC.png)

Find the missclassification for every split on X1|This is the best split
:-------------------------:|:-------------------------:
![image.png](https://miro.medium.com/max/514/0*NLvKQda-yuqa4Wee.png)|![image.png](https://miro.medium.com/max/491/0*2Hu0AMYGLXqgEMhV.png)

Find the next split | Finished
:-------------------------:|:-------------------------:
![image.png](https://miro.medium.com/max/510/0*zV-l2CROY1ZP_3qA.png)|![image.png](https://miro.medium.com/max/497/0*uGZjGt6N7BRxb46Q.png)

Resulting tree
![image.png](https://miro.medium.com/max/504/0*Z66SWGnP7OqSLuyC.png)


source:
https://towardsdatascience.com/learn-how-decision-trees-are-grown-22bc3d22fb51

Above our cost function we minimized was the number of missclassification
Two other common ones are gini impurity and information gain (entropy)

![image.png](https://miro.medium.com/max/1204/0*Nsuwaq2Padpbdz9Y.png)

It is out of the scope of this course to explain the math behind it
but a takehome message key difference between gini and entropy would be:

Gini: the feature with a lower Gini index is chosen for a split
Entropy: The feature with the largest information gain should is used as the root node

Further read:
https://blog.clairvoyantsoft.com/entropy-information-gain-and-gini-index-the-crux-of-a-decision-tree-99d0cdc699f4
https://towardsdatascience.com/gini-index-vs-information-entropy-7a7e4fed3fcb

##### Load Libraries
"""

import sys

if 'google.colab' in sys.modules:
  !pip install -q dtreeviz

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.datasets import load_breast_cancer

from sklearn.tree import DecisionTreeClassifier,plot_tree

from sklearn.model_selection import train_test_split, \
  GridSearchCV,RandomizedSearchCV

from sklearn.feature_selection import SelectFromModel

from sklearn.metrics import accuracy_score,confusion_matrix,classification_report

from dtreeviz.trees import *

#custom functions
from functools import wraps
from time import time
def timeit(func):
    @wraps(func)
    def _time_it(*args, **kwargs):
        start = int(round(time() * 1000))
        try:
            return func(*args, **kwargs)
        finally:
            end_ = int(round(time() * 1000)) - start
            print(f"Total execution time: {end_ if end_ > 0 else 0} ms")
    return _time_it

"""##### Exploring the breast cancer dataset"""

dataset=load_breast_cancer()

print ("daset.data");print()
print("type");print(type(dataset.data)); print()
print("shape");print(dataset.data.shape);print()
print("first 2 rows of the dataset:");print()
dataset.data[:2]

"""Machine learning terminology

*  row=an observation (also known as: sample,record)
*  column= feature (also known as: predictor, attribute, independent variable)

"""

print("features:"); print (dataset.feature_names);print()

print("dataset.target")
print("integers representing the diagnosis of each observation");print()
print("shape");print(dataset.target.shape);print()

print("dataset.target_names")
print("encoding scheme for diagnosis: 0 = malignant, 1 = benign")
print(dataset.target_names)

full_df = pd.DataFrame(dataset.data, columns=dataset.feature_names)
full_df["diagnosis"] = dataset.target

code2diagnosis={0:"malignant",1:"benign"}
full_df["diagnosis_name"] = full_df["diagnosis"].map(code2diagnosis)
full_df

"""Before we doing anything just a short distribution check of benign vs malignant tumors"""

sns.countplot(x = full_df["diagnosis_name"]) 
B, M = full_df["diagnosis_name"].value_counts()
print('Number of Benign: ',B)
print('Number of Malignant : ',M)

"""This is good news. It's always simpler to work on a classification problem when the different classes are equally represented. Here, the count ratio between malignant & benign tumors is close to 0.6, which is acceptable.

#### **Preprocess Data**

So above would now be a really nice "tidy" format which we could use for in depth explanatory analysis. However, since we want to use Machine Learning (i.e. scikit learn) we have to make sure that we fullfill all requirements:

Features and target: 
1. are separate objects
2. should be numeric
3. should be NumPy arrays
4. should have compatible shapes

Luckily in this dataset all the requirements are already fullfilled
So we only need to split the data in training and test set
"""

# Split dataset into training 80% and test (20%) set
test_size=0.2
x_train, x_test, y_train, y_test = train_test_split(
    dataset.data, dataset.target, test_size=test_size)

for t in [("training",x_train),("test",x_test)]:
  print(f"observations in the {t[0]} set:{len(t[1])}")

"""#### Create Decission Tree classifier"""

dct_clf = DecisionTreeClassifier(random_state=123456)
dct_clf.fit(x_train, y_train)

"""Tada! We have done it! We have officially trained our decission tree classifier! Now letâs play with it

#### Apply Classifier to Test Data
"""

# Apply the Classifier we trained to the test data (which, remember, it has never seen before)
targets_predicted=dct_clf.predict(x_test)
targets_predicted

"""What are you looking at above? Remember our encoding:
encoding scheme for diagnosis: 0 = malignant, 1 = benign. 

What the list of numbers above is showing you is what our model predicts each based on the 30 features shown in full_df

How confident is the classifier about each prediction? We can see that too.


"""

# View the predicted probabilities of the first 5 observations
dct_clf.predict_proba(x_test)[0:5]

"""There are two classes thus [ 1. , 0.] tells us that the classifier is certain that it is the first class.

If you would have for example a row with [ 0.9, 0.1. ]the classifier gives a 90% probability that the diagnosis belongs to the 2nd class and a 10% probability it belongs to the 2nd class. Because 90 is greater than 10, the classifier would predict the 2nd class.

"""

def appply_clf(clf,dataset,x_test,y_test):

  df=pd.DataFrame(x_test, columns=dataset.feature_names)
  df["target"]=y_test
  df["target_predicted"]=clf.predict(x_test)
  return df

test_df=appply_clf(dct_clf,dataset,x_test,y_test)
test_df.head()

"""That looks pretty good! At least for the first five observations.
So lets do a quick evaluation

"""

print(f'accuracy:{accuracy_score(y_test, dct_clf.predict(x_test))}')

"""Before we evaluate the classifier further lets explore and try to understand the used classifier

#### Explore the classifier

View Feature Importance

This is in my opinion the most powerful feature of tree based classification models. The importance of each feature can not only help you to understand better how the algorithm comes to its decision but also might give you insights into the underlying biology
"""

def get_feature_importance(clf,dataset):
  feature_imp = pd.Series(
      clf.feature_importances_,index=dataset.feature_names
      ).sort_values(ascending=False)

  sns.barplot(x=feature_imp, y=feature_imp.index)
  plt.xlabel('Feature Importance Score')
  plt.ylabel('Features')
  plt.title("Visualizing Important Features")
  plt.legend()
  plt.show()

  return (feature_imp)

feature_imp=get_feature_importance(dct_clf,dataset)

"""So the top 5 most important features in terms of making the split are"""

feature_imp[:5]

"""Lets start with plotting the tree"""

dtreeviz(dct_clf, x_train,y_train,target_name='diagnosis_name',
         feature_names=dataset.feature_names, 
         class_names=dataset.target_names.tolist())

"""#### Improve our classifier (by Hyperparameter tuning)

![param vs hyper params](https://editor.analyticsvidhya.com/uploads/67819opt.png)

model parameter is a configuration variable that is internal to the model and whose value can be estimated from data

model hyperparameter is a configuration that is external to the model and whose value cannot be estimated from data.

Above you see a quite "complex" tree lets see if we can simplify it?

`DecisionTreeClassifier` has a lot of configuration options (hyperparams).

|Option |Description|
--- | --- 
|max_depth|How many split will my tree be allowed to do?|
|criterion|loss function to train my tree (entropy/gini)|
|max_features|The number of features to consider when looking for the best split|
|max_leaf_nodes|Limit of how many leaves each split can produce|
|min_samples_leaf|The minimum number of samples required to produce a leaf|
|min_samples_split|Number of samples required in the leaf before splitting|

So it would be quite tedious to test them all on your own.
There are two ways to find the best hyperparams

![grid vs random search](https://miro.medium.com/proxy/1*ZTlQm_WRcrNqL-nLnx6GJA.png)

|Grid |Random|
--- | --- 
|![grid](https://miro.medium.com/max/363/0*4O9P0rwkJGmFr8r6)|![random](https://miro.medium.com/max/363/0*zOI72ajRHAq9W_uU)|
|! **does not scale well** because we try every combination of the set of parameters|we can try a broader range of values or hyperparameters within the same computation time|

Personal suggestion:
start with a randomized search to reduce the parameters space then do a grid search to select the optimal features within this space.
"""

params = {
    "max_depth"         : [*range(1, 30)],
    "criterion"         : ['entropy', 'gini'],
    "max_features"      : [*range(1, 30)],
    "max_leaf_nodes"    : [*range(1, 30)],
    'min_samples_leaf'  : [*range(1, 30)],
    'min_samples_split' : [*range(1, 30)]
}

@timeit
def search_hyperparams(clf,x,y,params,method):

  if method == "random":
    random_state=123456
    search = RandomizedSearchCV(clf, params, cv=3, random_state=123456,
                                return_train_score=True)

  else:
    search = GridSearchCV(clf, params, cv=3,return_train_score=True)

  search.fit(x,y)
  print(f"{method} - best params")
  print(search.best_params_)
  print()

  df=pd.DataFrame(search.cv_results_["params"])
  df["mean_test_score"]=search.cv_results_["mean_test_score"]
  df=df.sort_values(by='mean_test_score',ascending=False)

  clf2=search.best_estimator_
  clf2.fit(x,y)

  return clf2,df

dct_clf2,cv_df_rnd=search_hyperparams(
    dct_clf,x_train,y_train,params,"random")
cv_df_rnd

#dct_clf2,cv_df_grid=search_hyperparams(dct_clf,x_train,y_train,params,"grid")
#cv_df_grid

f,ax = plt.subplots(figsize=(10, 10))
cv_df_rnd=cv_df_rnd.drop(["mean_test_score"],axis=1)
cv_df_rnd.hist(ax=ax,bins=[1,5,10,15,20,25,30])
plt.show()

"""Based on the plots above and best params we prepare a grid search"""

params = {
    "max_depth"         : [10,15,20],
    "criterion"         : ['entropy', 'gini'],
    "max_features"      : [5,10,15,20,30],
    "max_leaf_nodes"    : [2,15,20,30],
    'min_samples_leaf'  : [2,25,30],
    'min_samples_split' : [20,25]}

best_params=dct_clf2.get_params()

for key in best_params:
  if key in params:
    params[key].append(best_params[key])
    params[key]=list(set(params[key]))

print(params)

dct_clf2,cv_df_grid=search_hyperparams(dct_clf,x_train,y_train,params,"grid")
cv_df_grid

"""So lets see how our tree has changed"""

dtreeviz(dct_clf2, x_train,y_train,target_name='diagnosis_name',
         feature_names=dataset.feature_names, 
         class_names=dataset.target_names.tolist())

"""Indeed much simpler as our old tree but before we see how this impacts our performance. So again a quick evaluation"""

print("accurcacy:")
for t in [("dct_clf",dct_clf),("dct_clf2",dct_clf2)]:
  print(f'{t[0]}:{accuracy_score(y_test, t[1].predict(x_test))}')

"""Only a small dip in performance eventhough having a much simpler tree.
So lets evaluate it in depth

#### Evaluate our classifiers
"""

def get_clf_performance(name,clf,x_train,y_train,x_test,y_test):
  print(name)
  print("classification report")
  y_test_pred = clf.predict(x_test)
  print(classification_report(y_test, y_test_pred))

for t in [("dct_clf",dct_clf),("dct_clf2",dct_clf2)]:
  get_clf_performance(t[0],t[1],x_train,y_train,x_test,y_test)

"""If you compare this to our classfier in the beginning they perform similar but we managed to greatly simplify it 

However, this doesnât really tell us anything about where weâre doing well. A useful technique for visualising performance is the confusion matrix.

This is simply a matrix whose diagonal values are true positive counts, while off-diagonal values are false positive and false negative counts for each class against the other.

OK sorry that might have confused you (pun intended ;P)

Hopefully that is more easy

anything on the diagonal was classified correctly and anything off the diagonal was classified incorrectly.

"""

def get_clf_cm(name,clf,target_names,x_train,y_train,x_test,y_test):
  
  print(name)
  cm = pd.DataFrame(
        confusion_matrix(y_test, clf.predict(x_test)),
        columns=target_names, 
        index=target_names
        )
  sns.heatmap(cm, annot=True)
  plt.xlabel("predicted")
  plt.ylabel("truth")
  plt.show()

for t in [("dct_clf",dct_clf),("dct_clf2",dct_clf2)]:
  get_clf_cm(t[0],t[1],dataset.target_names,x_train,y_train,x_test,y_test)

"""The initial (but more complex) classifier perfoms in terms of missclassification better but we made it more simple so depending on the use case it might be beneficial to switch to the easier classifier.

For the highly motivated participants:
Fell free to further tune the hyper parameters and let me know when you managed to improve it!

#### Excercise

Apply everything you have learned on the wine dataset.
Feel free to use my mini functions to save some line of codes.
Do not stress about any time limit I just want you to get familiar
with the scikit-learn commands there is no need to finish it.
Just try to get as far as possible ;D
"""

from sklearn.datasets import load_wine

"""##### Solution"""

from sklearn.datasets import load_wine
dataset = load_wine()

x_train, x_test, y_train, y_test = train_test_split(dataset.data,dataset.target)

wine_clf = DecisionTreeClassifier(random_state=123456)
wine_clf.fit(x_train, y_train)

get_clf_performance("wine_clf",wine_clf,x_train,y_train,x_test,y_test)

dtreeviz(wine_clf, x_train,y_train,target_name='wine_class',
         feature_names=dataset.feature_names, 
         class_names=dataset.target_names.tolist())

test_df=appply_clf(wine_clf,dataset,x_test,y_test)
test_df.head()

get_feature_importance(wine_clf,dataset)

wine_clf2 = DecisionTreeClassifier(random_state=123456)
params = {
    "max_depth"         : [1,2,3,4,5,10],
    "criterion"         : ['entropy', 'gini'],
    "max_features"      : [0.5, 1],
    "max_leaf_nodes"    : [2,3,5],
    'min_samples_leaf'  : [2,3,5],
    'min_samples_split' : [2,3,5]
}
gs = GridSearchCV(wine_clf2, params, cv=3)
gs.fit(x_train, y_train)
print(gs.best_params_)
print()
wine_clf2=gs.best_estimator_
wine_clf2.fit(x_train,y_train)
wine_clf2

dtreeviz(wine_clf, x_train,y_train,target_name='wine_class',
         feature_names=dataset.feature_names, 
         class_names=dataset.target_names.tolist())

for t in [("wine_clf",wine_clf),("wine_clf2",wine_clf2)]:
  get_clf_performance(t[0],t[1],x_train,y_train,x_test,y_test)

for t in [("wine_clf",wine_clf),("wine_clf2",wine_clf2)]:
  get_clf_cm(t[0],t[1],dataset.target_names,x_train,y_train,x_test,y_test)

"""## Random Forest

Random Forest combines the results of many different decision trees to make the best possible decisions.

![image.png](https://upload.wikimedia.org/wikipedia/commons/7/76/Random_forest_diagram_complete.png)

#### Load Libraries
"""

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.datasets import load_breast_cancer

from sklearn.ensemble import RandomForestClassifier

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.feature_selection import SelectFromModel

from sklearn.metrics import accuracy_score,confusion_matrix,classification_report

"""To compare it to our decision tree classifier we take again the breast cancer dataset"""

dataset = load_breast_cancer()

"""#### Preprocess Data"""

# Split dataset into training 80% and test (20%) set
test_size=0.2
x_train, x_test, y_train, y_test = train_test_split(
    dataset.data, dataset.target, test_size=test_size)

for t in [("training",x_train),("test",x_test)]:
  print(f"observations in the {t[0]} set:{len(t[1])}")

"""#### Create random forest classifier"""

num_of_random_trees=100
rf_clf = RandomForestClassifier(
    n_estimators=num_of_random_trees,
    oob_score=True, random_state=123456)

# Train the Classifier
rf_clf.fit(x_train, y_train)

"""#### **Apply Classifier To Test Data**

"""

# View the predicted probabilities of the first 5 observations
for t in [("dct_clf2",dct_clf2),("rf_clf",rf_clf)]:
  print(t[0]);print(t[1].predict_proba(x_test)[0:5]);print()

test_df=appply_clf(rf_clf,dataset,x_test,y_test)
test_df.head()

"""Again looks pretty good! At least for the first five observations.
Lets do a quick evaluation and comparison against the decission tree results
"""

print("accuracy")
for t in [("dct_clf2",dct_clf2),("rf_clf",rf_clf)]:
  print(f'{t[0]}: {accuracy_score(y_test, t[1].predict(x_test))}')

"""This first impression is confirmed by the quite similar accuracy scores

#### Explore the classifier
"""

feature_imp=get_feature_importance(rf_clf,dataset)
feature_imp

"""No surprise for the random forest also worst concave points, worst area...
are the most important features.

#### Improve our classifier

Of course we could also search again for the best parameters but this
time I want to show you the power of feature selection

For this we have to explore the breast cancer set a little bit furter.
However, before we can do this we first have to normalize thes dataset:
"""

#normalization by standardization
df = full_df.drop(["diagnosis","diagnosis_name"],axis = 1 )
df_normal = (df - df.mean()) / (df.std())
df_normal

"""Now we could do for example a violin plot to see which features might be especial usefule to distinguish between malignant and benign

![violin](https://miro.medium.com/max/520/1*TTMOaNG1o4PgQd-e8LurMg.png)

violin plot = combination of the box plot with a kernel density plot. 

The main advantage of the violin plot over the box plot is that it on top shows the entire distribution of the data. 
"""

def violin_plot(target,df):
    data = pd.concat([target,df],axis=1)
    data = pd.melt(data, id_vars="diagnosis_name", 
                   var_name="features", value_name='value')
    plt.figure(figsize=(30,10))
    sns.violinplot(x="features", y="value", hue="diagnosis_name", 
                   data=data, split=True, inner="quart")
    plt.xticks(rotation=45)

violin_plot(full_df["diagnosis_name"],df_normal)

"""In fact, there is a good separation between several features like e.g.
mean radius, mean perimeter etc., as the third quartile of benign tumors is below the first quartile of malignant tumors.

On top you could also observe that some violin plots look similar to each other
like e.g. worst radius and worst perimeter.
This could indicate that these two features are correlated.

In order to test this you can do a joint plot
"""

sns.jointplot(data=df_normal,x="worst radius",y="worst perimeter",kind="reg")
plt.show()

"""indeed these two are strongly correlated so you could drop one these features

To check for other existing correllations we can use the heatmap function of seaborn (see below)
"""

def get_corr(df,plot=True,figsize=(4,4)):
  corr=df.corr().abs()
  
  #mask for the upper triangle
  mask = np.triu(np.ones_like(corr, dtype=bool))
  
  if plot:
    f,ax = plt.subplots(figsize=figsize)
    sns.heatmap(corr,mask=mask,linewidths=.5, fmt= '.1f', annot=True)
    plt.show()
  
  return corr

corr=get_corr(df_normal,plot=True,figsize=(18,18))

"""You could use this matrix to find the highly correlating features by hand
or you can make your life more easy by extracting the highly correlated features programmatically
"""

def extract_high_corr(corr,cutoff):
  #https://www.dezyre.com/recipes/drop-out-highly-correlated-features-in-python
  upper_triangular = corr.where(np.triu(np.ones(corr.shape),k=1)
                                .astype(np.bool))
  
  high_corr_cols=[c for c in upper_triangular.columns if 
                  any(upper_triangular[c] > cutoff)]
  
  print("highly correlated features")
  print(high_corr_cols)

  return high_corr_cols

high_corr=get_corr(df_normal[extract_high_corr(corr,0.65)],figsize=(10,10))

"""In the corrplot above it is at least a little bit more easy to figure out what is correlating with what. You can now do an educated guess (based on your biology knowledge)
which features can be replaced by what e.g.

worst radius, worst perimeter and worst area -> I choose area_worst

or you could take the feature with the highest feature importance as an replacement
"""

def replace_features(feature_imp,features):
  replacement = feature_imp[features].nlargest(n=1).index[0]
  drop_features=list(set(features).symmetric_difference([replacement]))
  print (f"features to be dropped:{drop_features} replaced by '{replacement}'")

  return drop_features

 #["mean radius","mean perimeter","mean area"]
 #["worst radius","worst perimeter","worst area"],
 #["worst compactness", "worst concavity","worst concave points"],
 #["mean compactness","mean concavity","mean concave points"],
 #["radius error","perimeter error","area error"]


features=[
 ["worst radius","worst perimeter","worst area"]
]

drop_features=[]
for features_list in features:
  drop_features.extend(replace_features(feature_imp,features_list))

df_normal2 = df_normal.drop(drop_features,axis = 1)
print(f"columns {len(df_normal2.columns)}")
df_normal2.head()

"""So lets check how much correlation is left"""

corr2=get_corr(df_normal2,plot=True,figsize=(18,18))

"""We reduced the number of features from 30 to 20 and obtained a heatmap almost uncorrelated so convert it back into a training set and see how we it now performs"""

test_size=0.2
x_train_uncorr, x_test_uncorr, y_train_uncorr, y_test_uncorr = train_test_split(
    df_normal2, dataset.target, test_size=test_size)

num_of_random_trees=100
rf_clf2 = RandomForestClassifier(
    n_estimators=num_of_random_trees,
    oob_score=True, random_state=123456)

# Train the Classifier
rf_clf2.fit(x_train_uncorr, y_train_uncorr)

print("accuracy")
for t in [("dct_clf2",dct_clf2),("rf_clf",rf_clf)]:
  print(f'{t[0]}: {accuracy_score(y_test, t[1].predict(x_test))}')

print(f'rf_clf2: {accuracy_score(y_test_uncorr, rf_clf2.predict(x_test_uncorr))}')

"""So looks indeed quite promising but we still have 20 features so lets see if we can further reduce its complexity by taking only the features over a certain feature importance threshold"""

# Create a selector object that will use our classifier to identify
# features that have an importance of more than 0.04
sfm = SelectFromModel(rf_clf2, threshold=0.04)

# Train the selector
sfm.fit(x_train_uncorr, y_train_uncorr)

for feature_list_index in sfm.get_support(indices=True):
    print(dataset.feature_names[feature_list_index])

"""Create A Data Subset With Only The Most Important Features"""

x_train_important = sfm.transform(x_train_uncorr)
x_test_important = sfm.transform(x_test_uncorr)

"""Train A New Random Forest Classifier Using Only Most Important Features"""

# Create a new random forest classifier for the most important features
rf_clf3 = RandomForestClassifier(random_state=123456)

# Train the new classifier on the new dataset containing the most important features
rf_clf3.fit(x_train_important, y_train_uncorr)

print("accurcacy")
for t in [("dct_clf2",dct_clf2),("rf_clf",rf_clf)]:
  print(f'{t[0]}: {accuracy_score(y_test, t[1].predict(x_test))}')

print(f'rf_clf2: {accuracy_score(y_test_uncorr, rf_clf2.predict(x_test_uncorr))}')
print(f'rf_clf3:{accuracy_score(y_test_uncorr, rf_clf3.predict(x_test_important))}')

"""As expected our original model which contained all four features performs
slightly better then our feature selected model. So for a small cost in accuracy we halved the number of features in the model

#### **Evaluate our classifiers**
"""

for t in [("dct_clf2",dct_clf2),("rf_clf",rf_clf)]:
  get_clf_performance(t[0],t[1],x_train,y_train,x_test,y_test)

get_clf_performance(
    "rf_clf2",rf_clf2,x_train_uncorr,y_train,x_test_uncorr,y_test_uncorr)

get_clf_performance(
    "rf_clf3",rf_clf3,x_train_important,y_train,x_test_important,y_test_uncorr)

"""Performance stats look promising but better evaluate it in action!"""

for t in [("dct_clf2",dct_clf2),("rf_clf",rf_clf)]:
  get_clf_cm(t[0],t[1],dataset.target_names,x_train,y_train,x_test,y_test)

get_clf_cm(
    "rf_clf2",rf_clf2,dataset.target_names,
    x_train_uncorr,y_train_uncorr,x_test_uncorr,y_test_uncorr)

get_clf_cm(
    "rf_clf3",rf_clf3,dataset.target_names,
    x_train_important,y_train_uncorr,x_test_important,y_test_uncorr)

"""#### Exercise

Apply everything you have learned above.
I am especially interested about your feature selection strategy!
"""

from sklearn.datasets import load_breast_cancer
data = load_breast_cancer()

"""### Solution"""

# Split dataset into training 80% and test (20%) set
test_size=0.2
ex_x_train, ex_x_test, ex_y_train, ex_y_test = train_test_split(
    dataset.data, dataset.target, test_size=test_size)

for t in [("training",x_train),("test",x_test)]:
  print(f"observations in the {t[0]} set:{len(t[1])}")

num_of_random_trees=100
rf_clf4 = RandomForestClassifier(
    n_estimators=num_of_random_trees,
    oob_score=True, random_state=123456)

# Train the Classifier
rf_clf4.fit(ex_x_train, ex_y_train)

print("accuracy")
print(f'rf_clf4: {accuracy_score(y_test, rf_clf4.predict(x_test))}')

feature_imp=get_feature_importance(rf_clf,dataset)
feature_imp

full_df = pd.DataFrame(dataset.data, columns=dataset.feature_names)
full_df["diagnosis"] = dataset.target

code2diagnosis={0:"malignant",1:"benign"}
full_df["diagnosis_name"] = full_df["diagnosis"].map(code2diagnosis)
full_df

#normalization by standardization
df = full_df.drop(["diagnosis","diagnosis_name"],axis = 1 )
df_normal = (df - df.mean()) / (df.std())
df_normal

violin_plot(full_df["diagnosis_name"],df_normal)

sns.jointplot(data=df_normal,x="worst radius",y="worst perimeter",kind="reg")
plt.show()

corr=get_corr(df_normal,plot=True,figsize=(18,18))

high_corr=get_corr(df_normal[extract_high_corr(corr,0.65)],figsize=(10,10))

features=[
 ["worst radius","worst perimeter","worst area"],
 ["mean radius","mean perimeter","mean area"],
 ["worst compactness", "worst concavity","worst concave points"],
 ["mean compactness","mean concavity","mean concave points"],
 ["radius error","perimeter error","area error"]
]

drop_features=[]
for features_list in features:
  drop_features.extend(replace_features(feature_imp,features_list))

ex_df_normal2 = df_normal.drop(drop_features,axis = 1)
print(f"columns {len(ex_df_normal2.columns)}")
ex_df_normal2.head()

corr2=get_corr(ex_df_normal2,plot=True,figsize=(18,18))

test_size=0.2
ex_x_train_uncorr, ex_x_test_uncorr, ex_y_train_uncorr, ex_y_test_uncorr = train_test_split(
    df_normal2, dataset.target, test_size=test_size)

num_of_random_trees=100
rf_clf5 = RandomForestClassifier(
    n_estimators=num_of_random_trees,
    oob_score=True, random_state=123456)

# Train the Classifier
rf_clf5.fit(ex_x_train_uncorr, ex_y_train_uncorr)

print("accuracy")
for t in [("dct_clf2",dct_clf2),("rf_clf",rf_clf)]:
  print(f'{t[0]}: {accuracy_score(y_test, t[1].predict(x_test))}')

print(f'rf_clf2: {accuracy_score(y_test_uncorr, rf_clf2.predict(x_test_uncorr))}')
print(f'rf_clf3:{accuracy_score(y_test_uncorr, rf_clf3.predict(x_test_important))}')

print(f'rf_clf4:{accuracy_score(ex_y_test, rf_clf4.predict(ex_x_test))}')
print(f'rf_clf5: {accuracy_score(ex_y_test_uncorr, rf_clf5.predict(ex_x_test_uncorr))}')

# Create a selector object that will use our classifier to identify
# features that have an importance of more than 0.04
sfm = SelectFromModel(rf_clf5, threshold=0.04)

# Train the selector
sfm.fit(ex_x_train_uncorr, ex_y_train_uncorr)

for feature_list_index in sfm.get_support(indices=True):
    print(dataset.feature_names[feature_list_index])

ex_x_train_important = sfm.transform(ex_x_train_uncorr)
ex_x_test_important = sfm.transform(ex_x_test_uncorr)

# Create a new random forest classifier for the most important features
rf_clf6 = RandomForestClassifier(random_state=123456)

# Train the new classifier on the new dataset containing the most important features
rf_clf6.fit(ex_x_train_important, ex_y_train_uncorr)

print("accuracy")
for t in [("dct_clf2",dct_clf2),("rf_clf",rf_clf)]:
  print(f'{t[0]}: {accuracy_score(y_test, t[1].predict(x_test))}')

print(f'rf_clf2: {accuracy_score(y_test_uncorr, rf_clf2.predict(x_test_uncorr))}')
print(f'rf_clf3:{accuracy_score(y_test_uncorr, rf_clf3.predict(x_test_important))}')

print(f'rf_clf4:{accuracy_score(ex_y_test, rf_clf4.predict(ex_x_test))}')
print(f'rf_clf5: {accuracy_score(ex_y_test_uncorr, rf_clf5.predict(ex_x_test_uncorr))}')
print(f'rf_clf6: {accuracy_score(ex_y_test_uncorr, rf_clf6.predict(ex_x_test_important))}')

for t in [("dct_clf2",dct_clf2),("rf_clf",rf_clf)]:
  get_clf_cm(t[0],t[1],dataset.target_names,x_train,y_train,x_test,y_test)

get_clf_cm(
    "rf_clf2",rf_clf2,dataset.target_names,
    x_train_uncorr,y_train_uncorr,x_test_uncorr,y_test_uncorr)

get_clf_cm(
    "rf_clf3",rf_clf3,dataset.target_names,
    x_train_important,y_train_uncorr,x_test_important,y_test_uncorr)

get_clf_cm(
    "rf_clf4",rf_clf3,dataset.target_names,
    ex_x_train_important,ex_y_train_uncorr,ex_x_test_important,ex_y_test_uncorr)

get_clf_cm(
    "rf_clf5",rf_clf3,dataset.target_names,
    ex_x_train_important,ex_y_train_uncorr,ex_x_test_important,ex_y_test_uncorr)

get_clf_cm(
    "rf_clf6",rf_clf3,dataset.target_names,
    ex_x_train_important,ex_y_train_uncorr,ex_x_test_important,ex_y_test_uncorr)

"""If you want to dig deeper into feature here is my resource for this section: https://www.kaggle.com/quantumofronron/breast-cancer-data-set-feature-selection

## Support Vector Machines

A support vector machine (SVM) uses algorithms to train and classify data within degrees of polarity, taking it to a degree beyond X/Y prediction. 

For a simple visual explanation, weâll use two tags: red and green, with two data features: X and Y, then train our classifier to output an X/Y coordinate as either red or green.


# ![image.png](https://miro.medium.com/max/724/0*INqwwHXgTabQx7wM.png)

#### Load Libraries
"""

import random

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn import svm
from sklearn.metrics import accuracy_score

"""#### generate a random dataset"""

def generate_random_list(seed):
    lst=[]
    for t in seed:
      start,end,ndigits=t
      lst.append(round(random.uniform(start,end),ndigits))
    return lst

def generate_random_dataset(size):
    # taken from
    #https://towardsdatascience.com/support-vector-machines-explained-with-python-examples-cb65e8172c85
    seed_x = [(0,2.5,1),(1,5,2),(3,5,2)]
    seed_y = [(0,20,1),(20,25,2),(5,25,2)]
    x,y,target = [],[],[]
    for i in range(size):
      x.extend(generate_random_list(seed_x))
      y.extend(generate_random_list(seed_y))
      target.extend([0,1,1])

    df = pd.DataFrame(list(zip(x,y,target)),columns=['x', 'y','target'])
    return df

# Generate dataset
dataset_size = 100
df = generate_random_dataset(dataset_size)
features = df[['x', 'y']].to_numpy()
label = df['target'].to_numpy()
target_names = np.unique(label)

print("generated dataset")
df.head()

# Split dataset into training (80%) and testing (20%) set
x_train, x_test, y_train, y_test = train_test_split(
    features, label, test_size=0.2)

for t in [("training",x_train),("test",x_test)]:
  print(f"observations in the {t[0]} set:{len(t[1])}")

# Plotting the training set
fig, ax = plt.subplots(figsize=(12, 7))
ax.scatter(x_train[:,0],x_train[:,1])
plt.show()

"""Thereâs a little space between the two groups of data points. But closer to the center, itâs not clear which data point belongs to which class.

#### Fit a linear SVM

A linear curve might be a good candidate to separate these classes. So letâs fit the SVM
"""

svm_lin_clf = svm.SVC(kernel='linear',random_state=123456)
svm_lin_clf.fit(x_train, y_train)

def plot_decision_boundaries(clf,features,x_train,y_train):
  fig, ax = plt.subplots(figsize=(12, 7))
  
  # Create grid to evaluate classifier
  xx = np.linspace(-1, max(features[:,0]) + 1, len(x_train))
  yy = np.linspace(0, max(features[:,1]) + 1, len(y_train))
  YY, XX = np.meshgrid(yy, xx)
  xy = np.vstack([XX.ravel(), YY.ravel()]).T
  
  # Assigning different colors to the classes
  colors = np.where(y_train == 1, '#8C7298', '#4786D1')
  
  # Plot the dataset
  ax.scatter(
      x_train[:,0],
      x_train[:,1],
      c=colors)
  
  # Get the separating hyperplane
  Z = clf.decision_function(xy).reshape(XX.shape)
  
  # Draw the decision boundary and margins
  ax.contour(XX, YY, Z, colors='k', 
             levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])
  
  # Highlight support vectors with a circle around them
  ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],
             s=100, linewidth=1, facecolors='none', edgecolors='k')
  
  print("Dataset after classification, with decision boundary (full line)")
  print("margin (dashed lines) and support vectors marked with a circle.")
  plt.show()

plot_decision_boundaries(svm_lin_clf,features,x_train,y_train)

"""If we calculate the accuracy of this model against the testing set we get a good result, granted the dataset is very small and generated at random."""

print(f'accuracy: {accuracy_score(y_test, svm_lin_clf.predict(x_test))}')

get_clf_cm("svm_lin_clf",svm_lin_clf,target_names,x_train,y_train,x_test,y_test)

"""#### Fit a quadratic SVM

The accuracy is good, but let's see if a more complex approach can return an even better result. To fit an SVM with a polynomial kernel we just need to update the kernel parameter.
"""

svm_poly_clf = svm.SVC(kernel='poly', degree=2,random_state=123456)
svm_poly_clf.fit(x_train, y_train)

plot_decision_boundaries(svm_poly_clf,features,x_train,y_train)

print("accuracy")
for t in [("svm_lin_clf",svm_lin_clf),("svm_poly_clf",svm_poly_clf)]:
  print(f'{t[0]}: {accuracy_score(y_test, t[1].predict(x_test))}')

"""So it turns out that for this problem a simpler model, an SVM with a linear kernel, was the best solution."""

for t in [("svm_lin_clf",svm_lin_clf),("svm_poly_clf",svm_poly_clf)]:
  get_clf_cm(t[0],t[1],target_names,x_train,y_train,x_test,y_test)

"""### Exercise

Ok that was easy because we only did a binary classification lets
try our best again with our beloved cancer dataset

###### Solution
"""

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn import svm
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report

from sklearn.datasets import load_breast_cancer

dataset=load_breast_cancer()

test_size=0.2
x_train, x_test, y_train, y_test = train_test_split(
    dataset.data,dataset.target,test_size=test_size)

svm_clf = svm.SVC(kernel='linear')
svm_clf.fit(x_train, y_train)

test_df=appply_clf(svm_clf,dataset,x_test,y_test)
test_df.head()

accuracy = accuracy_score(y_test, svm_clf.predict(x_test))
print(f'Mean accuracy score: {accuracy:.3}')

get_clf_cm("svm_clf",svm_clf,dataset.target_names,x_train,y_train,x_test,y_test)

get_clf_performance("svm_clf",svm_clf,x_train,y_train,x_test,y_test)

#Bonus: Feature importance
from matplotlib import pyplot as plt
from sklearn import svm

def f_importances(coef, names):
    imp,names = zip(*sorted(zip(coef,names)))
    plt.barh(range(len(names)), imp, align='center')
    plt.yticks(range(len(names)), names)
    plt.show()

f,ax = plt.subplots(figsize=(10,10))
f_importances(svm_clf.coef_[0], dataset.feature_names)
plt.show()

"""Note:
Getting the feature importance in a non-linear SVM is impossible:

In linear SVM the resulting separating plane is in the same space as your input features. Therefore its coefficients can be viewed as weights of the input's "dimensions".

In other kernels, the separating plane exists in another space - a result of kernel transformation of the original space. Its coefficients are not directly related to the input space
"""